{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "list_of_lists = [[1,2,3], [4,5,6]]\n",
    "print(list_of_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1.],\n",
      "        [2., 3.],\n",
      "        [4., 5.]])\n"
     ]
    }
   ],
   "source": [
    "# initialize the tensor\n",
    "data = torch.tensor([[0,1], \n",
    "                     [2,3], \n",
    "                     [4,5]], dtype=torch.float32)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "zeros = torch.zeros(2,5)\n",
    "ones = torch.ones(2,5)\n",
    "print(zeros)\n",
    "print(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "rr = torch.arange(1,10)\n",
    "print(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = tensor([[1, 2],\n",
      "        [2, 3],\n",
      "        [4, 5]])\n",
      "B = tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "C = A@B = tensor([[11, 14, 17, 20],\n",
      "        [17, 22, 27, 32],\n",
      "        [29, 38, 47, 56]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[1,2], [2,3], [4,5]])\n",
    "B = torch.tensor([[1,2,3,4],[5,6,7,8]])\n",
    "C = A @ B # or equivalently A.matmul(B)\n",
    "print(f\"A = {A}\")\n",
    "print(f\"B = {B}\")\n",
    "print(f\"C = A@B = {C}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([1,2,3])\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1,2,3], [4,5,6]]) @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rr = tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n",
      "Shape before reshaping: torch.Size([15])\n",
      "rr = tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12],\n",
      "        [13, 14, 15]])\n",
      "Shape after reshaping: torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# reshaping tensors\n",
    "rr =  torch.arange(1,16)\n",
    "print(f\"rr = {rr}\")\n",
    "print(f\"Shape before reshaping: {rr.shape}\")\n",
    "rr = rr.view(5,3)\n",
    "print(f\"rr = {rr}\")\n",
    "print(f\"Shape after reshaping: {rr.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 5]])\n",
      "[[1 0 5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# convert numpy array to torch tensor\n",
    "arr = np.array([[1,0,5]])\n",
    "data = torch.tensor(arr)\n",
    "print(data)\n",
    "\n",
    "# convert torch tensor to numpy array\n",
    "new_arr = data.numpy()\n",
    "print(new_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11., 12., 13., 14.],\n",
      "        [15., 16., 17., 18., 19., 20., 21.],\n",
      "        [22., 23., 24., 25., 26., 27., 28.],\n",
      "        [29., 30., 31., 32., 33., 34., 35.]])\n",
      "sum over columns: tensor([ 75.,  80.,  85.,  90.,  95., 100., 105.])\n",
      "sum over rows: tensor([ 28.,  77., 126., 175., 224.])\n",
      "average over rows: tensor([ 4., 11., 18., 25., 32.])\n",
      "sum over all elements of tensor: 630.0\n"
     ]
    }
   ],
   "source": [
    "# vectorized operations\n",
    "data = torch.arange(1, 36, dtype=torch.float32).reshape(5, 7)\n",
    "print(data)\n",
    "\n",
    "print(f\"sum over columns: {data.sum(dim=0)}\")\n",
    "print(f\"sum over rows: {data.sum(dim=1)}\")\n",
    "print(f\"average over rows: {data.mean(dim=1)}\")\n",
    "print(f\"sum over all elements of tensor: {data.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = \n",
      "tensor([[ 1.0000,  2.2000,  9.6000],\n",
      "        [ 4.0000, -7.2000,  6.3000]]), \n",
      "shape = torch.Size([2, 3])\n",
      "\n",
      "Average over rows: \n",
      "tensor([4.2667, 1.0333]), \n",
      "shape = torch.Size([2])\n",
      "Average over cols: \n",
      "tensor([ 2.5000, -2.5000,  7.9500]), \n",
      "shape = torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# Quiz\n",
    "A = torch.tensor([[1,2.2, 9.6], [4, -7.2, 6.3]])\n",
    "print(f\"A = \\n{A}, \\nshape = {A.shape}\")\n",
    "print()\n",
    "row_avg = A.mean(dim=1)\n",
    "print(f\"Average over rows: \\n{row_avg}, \\nshape = {row_avg.shape}\")\n",
    "col_avg = A.mean(dim=0)\n",
    "print(f\"Average over cols: \\n{col_avg}, \\nshape = {col_avg.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14]])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# tensor slicing\n",
    "matr = torch.arange(15).view(5,3)\n",
    "print(matr)\n",
    "print(matr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matr[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  3,  6,  9, 12])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matr[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matr[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1],\n",
       "        [ 3,  4],\n",
       "        [ 6,  7],\n",
       "        [ 9, 10],\n",
       "        [12, 13]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matr[:, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 7, 8])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matr[0:3][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 6,  7,  8],\n",
       "        [12, 13, 14]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access elements 0,2, and 4\n",
    "matr[[0,2,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access the 0th and 1st elements, each twice\n",
    "matr[[0,0,1,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert a tensor element into a python scalar value\n",
    "matr[0,0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000,  2.2000,  9.6000],\n",
      "        [ 4.0000, -7.2000,  6.3000]])\n",
      "first column = tensor([1., 4.])\n",
      "first row = tensor([1.0000, 2.2000, 9.6000])\n"
     ]
    }
   ],
   "source": [
    "#Exercise\n",
    "B = torch.tensor([[1, 2.2, 9.6], [4, -7.2, 6.3]])\n",
    "print(B)\n",
    "print(f\"first column = {B[:,0]}\")\n",
    "print(f\"first row = {B[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# set gradient tracking for this tensor\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "pp.pprint(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "# compute function of x\n",
    "y = 3 * x * x\n",
    "\n",
    "# backpropagate the gradients of y\n",
    "y.backward()\n",
    "\n",
    "# show the upstream/backpropagated gradient a x\n",
    "pp.pprint(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([24.])\n"
     ]
    }
   ],
   "source": [
    "# running backprop from a different tensor\n",
    "z = 3 * x * x\n",
    "z.backward() \n",
    "# note that the different backpropagated gradients have accumulated at x\n",
    "pp.pprint(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "torch.Size([2, 3, 4])\n",
      "tensor([[[ 0.6883, -0.0623],\n",
      "         [ 0.6883, -0.0623],\n",
      "         [ 0.6883, -0.0623]],\n",
      "\n",
      "        [[ 0.6883, -0.0623],\n",
      "         [ 0.6883, -0.0623],\n",
      "         [ 0.6883, -0.0623]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# linear layer\n",
    "\n",
    "# create inputs\n",
    "input = torch.ones(2,3,4) # 2 batches, 3 instances per batch, 4 features per instance\n",
    "print(input)\n",
    "print(input.shape)\n",
    "\n",
    "# make linear layer that will transform input of shape (N, *, H_in) into output of shape (N, *, H_out), `*` can be any arbitrary number of intermediate dimensions\n",
    "H_in = 4 # size of the last dimension of the input (e.g. number of features in an instance)\n",
    "H_out = 2 # size of last dimension of output (e.g. number of hidden units/features for each instance)\n",
    "linear = nn.Linear(H_in, H_out)\n",
    "\n",
    "linear_output = linear(input)\n",
    "# compute output = WX + b, where W and b are parameters of the linear layer\n",
    "print(linear_output)\n",
    "print(linear_output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0595,  0.1507,  0.2668, -0.2181],\n",
       "         [ 0.2509,  0.0537, -0.3941,  0.1925]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.4293, -0.1652], requires_grad=True)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show parameters of linear layer\n",
    "list(linear.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6656, 0.4844],\n",
      "         [0.6656, 0.4844],\n",
      "         [0.6656, 0.4844]],\n",
      "\n",
      "        [[0.6656, 0.4844],\n",
      "         [0.6656, 0.4844],\n",
      "         [0.6656, 0.4844]]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# applying activation function\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "output = sigmoid(linear_output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2660, 0.5987],\n",
      "         [0.2660, 0.5987],\n",
      "         [0.2660, 0.5987]],\n",
      "\n",
      "        [[0.2660, 0.5987],\n",
      "         [0.2660, 0.5987],\n",
      "         [0.2660, 0.5987]]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# sequential block of layers\n",
    "H_in = 4\n",
    "H_out = 2\n",
    "block = nn.Sequential(\n",
    "    nn.Linear(H_in,H_out),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "input = torch.ones(2,3,4)\n",
    "output = block(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating custom layers/modules as child classes extending from the nn.Module class\n",
    "\n",
    "# a single hidden-layer perceptron model\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "\n",
    "        # call base class constructor\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # define the layers in our model (final output size same as input size) \n",
    "        self.linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(self.hidden_size, self.input_size)\n",
    "        self.sigmoid = nn.Sigmoid()            \n",
    "        \n",
    "\n",
    "    # must also implement the forward function for our custom module\n",
    "    def forward(self, x):\n",
    "        linear_output = self.linear(x)\n",
    "        relu_output = self.relu(linear_output)\n",
    "        linear2_output = self.linear2(relu_output)\n",
    "        output = self.sigmoid(linear2_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2877, 0.3811, 0.5304, 0.3834, 0.6573],\n",
      "        [0.2345, 0.3950, 0.5593, 0.3409, 0.6938]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# sample input \n",
    "input = torch.randn(2, 5)\n",
    "\n",
    "# instantiate the model (with 3 hidden units)\n",
    "model = MultiLayerPerceptron(5, 3)\n",
    "\n",
    "# forward pass\n",
    "output = model(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('linear.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0899,  0.0124, -0.2547,  0.2632,  0.3785],\n",
       "          [ 0.2410,  0.1906, -0.3415,  0.2595,  0.4015],\n",
       "          [-0.3060,  0.3868, -0.1156,  0.0063, -0.3740]], requires_grad=True)),\n",
       " ('linear.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0.0698, 0.4089, 0.4340], requires_grad=True)),\n",
       " ('linear2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.4442,  0.0930, -0.5207],\n",
       "          [ 0.1719, -0.1546,  0.1177],\n",
       "          [-0.1057,  0.0786,  0.2135],\n",
       "          [-0.1152, -0.1813, -0.3345],\n",
       "          [-0.0785,  0.2821,  0.2971]], requires_grad=True)),\n",
       " ('linear2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.5736, -0.5603, -0.0146, -0.2613,  0.4611], requires_grad=True))]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspecting the parameters of our model\n",
    "list(model.named_parameters())\n",
    "\n",
    "# list(model.parameters()) # does not show name of layer each parameter belongs to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5637, -0.3139, -1.2994,  0.1776,  0.6535],\n",
      "        [ 1.0292,  2.2749, -1.0866, -1.5438, -0.8153],\n",
      "        [-0.2285,  1.4981,  3.0316,  2.2977,  0.3427],\n",
      "        [ 0.0385,  3.3077,  0.5022,  0.6523,  0.1872],\n",
      "        [ 2.1836,  0.9063, -0.5379,  1.4352,  0.5254],\n",
      "        [-0.4631,  1.5313,  2.4692,  0.6737, -0.0400],\n",
      "        [ 0.5188,  1.3091,  0.8802,  1.6411,  1.0992],\n",
      "        [ 1.5983, -1.5868, -0.5141,  1.6641,  1.4023],\n",
      "        [ 0.7643, -0.2611,  0.6698,  0.2939,  1.8008],\n",
      "        [ 1.1789,  1.0759, -1.0149,  0.9886,  2.1318]])\n"
     ]
    }
   ],
   "source": [
    "# define some dummy data of 1s and add some noise\n",
    "y = torch.ones(10, 5) # ground truth predictions\n",
    "x = y + torch.randn_like(y) # inputs are ground truth with noise added (i.task of our model is predict noise-free input, i.e. \"denoising\")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6354938745498657\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "model = MultiLayerPerceptron(5, 3)\n",
    "\n",
    "# instantiate an adam optimizer, pass in the parameters which need to be updated and specify learning_rate\n",
    "adam = optim.Adam(model.parameters(), lr=1.e-1)\n",
    "\n",
    "# define loss function, use predefined binary-crossentropy loss module \n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "# forward pass through our model\n",
    "y_pred = model(x)\n",
    "# compute loss\n",
    "loss = loss_function(y_pred, y)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model to acheive a smaller loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 0, training loss: 0.6354938745498657\n",
      "Iteration # 1, training loss: 0.5609316825866699\n",
      "Iteration # 2, training loss: 0.4782762825489044\n",
      "Iteration # 3, training loss: 0.37945666909217834\n",
      "Iteration # 4, training loss: 0.278301477432251\n",
      "Iteration # 5, training loss: 0.18868480622768402\n",
      "Iteration # 6, training loss: 0.12010882049798965\n",
      "Iteration # 7, training loss: 0.07230803370475769\n",
      "Iteration # 8, training loss: 0.04266883805394173\n",
      "Iteration # 9, training loss: 0.025564804673194885\n"
     ]
    }
   ],
   "source": [
    "# number of training epochs\n",
    "n_epochs = 10\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # always reset gradients to zero before doing backward pass in every iteration\n",
    "    adam.zero_grad()\n",
    "\n",
    "    # get model predictions\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # compute loss\n",
    "    loss = loss_function(y_pred, y)\n",
    "\n",
    "    # print stats\n",
    "    print(f\"Iteration # {epoch}, training loss: {loss}\")\n",
    "\n",
    "    # compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # take a step to optimize the weights\n",
    "    adam.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.6975,  0.8236,  0.4145,  0.7207,  0.6389],\n",
       "         [ 0.4801, -0.8034, -1.1236,  0.5743,  1.0574],\n",
       "         [-0.8147, -0.4146, -0.0251, -0.1509, -0.2488]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 1.0623,  0.5875, -0.3840], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[1.3533, 1.1272, 0.0325],\n",
       "         [0.6256, 1.1075, 0.3937],\n",
       "         [1.1118, 1.0860, 0.3375],\n",
       "         [0.5975, 0.5584, 0.2360],\n",
       "         [1.0359, 1.1111, 0.7857]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1.1711, 1.2947, 0.6222, 0.8151, 1.2717], requires_grad=True)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show parameters after training\n",
    "list(model.parameters()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9927, 0.9910, 0.9847, 0.9341, 0.9924],\n",
      "        [0.9643, 0.9069, 0.9143, 0.8524, 0.9477],\n",
      "        [0.9998, 0.9900, 0.9985, 0.9814, 0.9988],\n",
      "        [0.9994, 0.9849, 0.9968, 0.9726, 0.9976],\n",
      "        [1.0000, 0.9993, 0.9998, 0.9940, 0.9999],\n",
      "        [0.9972, 0.9699, 0.9890, 0.9477, 0.9925],\n",
      "        [0.9998, 0.9949, 0.9990, 0.9846, 0.9993],\n",
      "        [1.0000, 0.9999, 0.9999, 0.9964, 1.0000],\n",
      "        [0.9997, 0.9974, 0.9987, 0.9821, 0.9992],\n",
      "        [1.0000, 0.9998, 1.0000, 0.9971, 1.0000]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# lets check and see if the predictions are similar to the ground truth y (which contains all 1s)\n",
    "y_pred = model(x)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1212, -0.0696,  3.1144,  0.7191, -0.2049],\n",
      "        [ 0.9153,  1.2836,  1.5781,  1.4026,  0.2676],\n",
      "        [-0.1416,  0.7168,  1.0615,  1.3932,  0.6284],\n",
      "        [ 1.5002,  0.2637,  2.3183,  0.0832,  0.2265],\n",
      "        [ 0.0519,  2.4155,  1.7536,  2.3153,  1.2661],\n",
      "        [ 0.5324,  1.5999, -0.7706, -0.0296,  0.9372],\n",
      "        [ 1.1400,  0.3082, -0.3737,  1.0469,  0.8481],\n",
      "        [ 1.2724,  2.8108, -1.2418,  1.3649,  0.0582],\n",
      "        [ 1.8439,  0.5680, -0.9852, -1.0735,  2.0157],\n",
      "        [ 1.2354,  0.2178,  2.3096, -0.2292,  1.3061]])\n",
      "tensor([[0.9909, 0.9488, 0.9710, 0.9144, 0.9814],\n",
      "        [0.9994, 0.9848, 0.9968, 0.9724, 0.9976],\n",
      "        [0.9976, 0.9749, 0.9904, 0.9511, 0.9935],\n",
      "        [0.9973, 0.9701, 0.9891, 0.9479, 0.9925],\n",
      "        [0.9999, 0.9947, 0.9995, 0.9898, 0.9996],\n",
      "        [0.9989, 0.9912, 0.9959, 0.9675, 0.9974],\n",
      "        [0.9998, 0.9984, 0.9993, 0.9869, 0.9996],\n",
      "        [0.9999, 0.9963, 0.9993, 0.9869, 0.9995],\n",
      "        [0.9999, 0.9992, 0.9996, 0.9900, 0.9998],\n",
      "        [0.9980, 0.9741, 0.9916, 0.9545, 0.9941]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# create some test data and check how model performs on it (test data drawn from same distribution as training data)\n",
    "x_test = y + torch.randn_like(y)\n",
    "print(x_test)\n",
    "\n",
    "y_pred_test = model(x_test)\n",
    "print(y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model has learned how to filter out the noise from the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n_a3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
