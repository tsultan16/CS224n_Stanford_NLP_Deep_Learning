{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Project: Word Window Classification\n",
    "\n",
    "In this NLP task, we will train a model to recognize words within a sentence that correspond to the name of a `LOCATION`, e.g. in the sentence \"I went to France last year\", the word \"France\" is a LOCATION. We will use a window that scans over each word in a sentence and classifies the center word as either a LOCATION or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw dataset/corpus\n",
    "corpus = [\n",
    "    \"We always come to Paris\",\n",
    "    \"The professor is from Australia\",\n",
    "    \"I live in Stanford\",\n",
    "    \"He comes from Taiwan\",\n",
    "    \"The capital of Turkey is Ankara\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "We will tokenize each sentence into a list of words and make all words lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['we', 'always', 'come', 'to', 'paris'],\n",
      " ['the', 'professor', 'is', 'from', 'australia'],\n",
      " ['i', 'live', 'in', 'stanford'],\n",
      " ['he', 'comes', 'from', 'taiwan'],\n",
      " ['the', 'capital', 'of', 'turkey', 'is', 'ankara']]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    tokenized_sent = sentence.lower().split(\" \")\n",
    "    return tokenized_sent\n",
    "\n",
    "# create pre-processed training instances\n",
    "train_sentences = [preprocess_sentence(sentence) for sentence in corpus]\n",
    "pp.pprint(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1, 0, 1]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create labels for training instances, i.e. in each instance each word is labeled 1 if it's a location and 0 otherwise\n",
    "locations = ['paris', 'australia', 'stanford', 'taiwan', 'turkey', 'ankara']\n",
    "\n",
    "train_labels = [[1 if word in locations else 0 for word in sentence] for sentence in train_sentences]\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we create the vocabulary set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>',\n",
       " 'always',\n",
       " 'ankara',\n",
       " 'australia',\n",
       " 'capital',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'from',\n",
       " 'he',\n",
       " 'i',\n",
       " 'in',\n",
       " 'is',\n",
       " 'live',\n",
       " 'of',\n",
       " 'paris',\n",
       " 'professor',\n",
       " 'stanford',\n",
       " 'taiwan',\n",
       " 'the',\n",
       " 'to',\n",
       " 'turkey',\n",
       " 'we'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = set(word for sentence in train_sentences for word in sentence)\n",
    "# also add an unknown token '<unk>' for out-of-vocab words\n",
    "vocabulary.add(\"<unk>\")\n",
    "\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad the beginning of end of each sentence with a special token so that the window always contains the same number of words (even at beginning and end of a sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<pad>', 'we', 'always', 'come', 'to', 'paris', '<pad>', '<pad>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a special padding token\n",
    "vocabulary.add(\"<pad>\")\n",
    "\n",
    "# pad the beginning and end of each sentence\n",
    "def pad_sentence(sentence, window_size, pad_token=\"<pad>\"):\n",
    "    padding = [pad_token]*window_size\n",
    "    padded_sentence = padding + sentence + padding\n",
    "    return padded_sentence\n",
    "\n",
    "# example\n",
    "window_size = 2\n",
    "pad_sentence(train_sentences[0], window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index the words in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<unk>': 1,\n",
       " 'always': 2,\n",
       " 'ankara': 3,\n",
       " 'australia': 4,\n",
       " 'capital': 5,\n",
       " 'come': 6,\n",
       " 'comes': 7,\n",
       " 'from': 8,\n",
       " 'he': 9,\n",
       " 'i': 10,\n",
       " 'in': 11,\n",
       " 'is': 12,\n",
       " 'live': 13,\n",
       " 'of': 14,\n",
       " 'paris': 15,\n",
       " 'professor': 16,\n",
       " 'stanford': 17,\n",
       " 'taiwan': 18,\n",
       " 'the': 19,\n",
       " 'to': 20,\n",
       " 'turkey': 21,\n",
       " 'we': 22}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort the tokens so that padding token is the first one in the list\n",
    "ind2word = sorted(list(vocabulary))\n",
    "word2ind = {word:i for i,word in enumerate(ind2word)}\n",
    "word2ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert training sentences into sequences of word indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example sentence: ['i', 'live', 'in', 'argentina']\n",
      "converted to indices: [10, 13, 11, 1]\n",
      "sentence restored from indices: ['i', 'live', 'in', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_indices(sentence, word2ind):\n",
    "    indices = [word2ind.get(token, word2ind[\"<unk>\"]) for token in sentence]\n",
    "    return indices\n",
    "\n",
    "def indices_to_sentence(indices, ind2word):\n",
    "    sentence = [ind2word[ix] for ix in indices]\n",
    "    return sentence\n",
    "\n",
    "# example\n",
    "example_sentence = [\"i\", \"live\", \"in\", \"argentina\"]\n",
    "example_indices = sentence_to_indices(example_sentence, word2ind)\n",
    "sentence_restored = indices_to_sentence(example_indices, ind2word)\n",
    "\n",
    "print(f\"example sentence: {example_sentence}\")\n",
    "print(f\"converted to indices: {example_indices}\")\n",
    "print(f\"sentence restored from indices: {sentence_restored}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[22, 2, 6, 20, 15],\n",
       " [19, 16, 12, 8, 4],\n",
       " [10, 13, 11, 17],\n",
       " [9, 7, 8, 18],\n",
       " [19, 5, 14, 21, 12, 3]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now convert the training sentences to indices\n",
    "train_indices = [sentence_to_indices(s, word2ind) for s in train_sentences]\n",
    "train_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating embedding vectors using a pytorch `nn.Embedding` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-1.6204, -1.3235, -0.3489,  0.3866,  0.0549],\n",
       "         [-0.4126,  1.9121,  0.5493, -0.0802, -0.7693],\n",
       "         [ 1.6380,  2.0960, -0.9376, -0.2087,  0.3668],\n",
       "         [ 0.0562,  1.3598, -1.4467, -1.1859, -0.0695],\n",
       "         [ 0.6596,  0.4296, -0.7453, -0.9501,  0.1531],\n",
       "         [-0.1683,  1.4943, -1.0681,  2.1593,  1.1089],\n",
       "         [ 0.5112,  0.4445,  0.1515, -1.3416, -0.3067],\n",
       "         [ 1.5297, -1.7061, -1.0710, -0.4877, -0.7624],\n",
       "         [ 1.4409, -0.8704,  1.3974,  1.6310,  0.3263],\n",
       "         [ 0.6451, -0.6095,  1.9080,  0.3958, -0.2163],\n",
       "         [ 0.6885,  1.2283,  0.4844,  0.7660,  0.3550],\n",
       "         [-0.6197, -0.0463, -0.2521, -0.2903, -0.6396],\n",
       "         [-0.8334, -0.3446, -0.3361,  1.4775, -0.2071],\n",
       "         [-1.9010, -0.6506, -1.7711, -1.4585, -0.9989],\n",
       "         [ 1.3669,  0.9970, -0.0068,  0.1039,  1.9751],\n",
       "         [ 0.3550,  0.4291, -0.1219, -1.6771,  0.7565],\n",
       "         [-0.6452, -0.8114, -1.2496,  0.2212, -0.1379],\n",
       "         [ 1.0280,  0.5036,  0.3202, -0.4564,  0.3102],\n",
       "         [-1.4933, -0.2315,  1.1445,  0.4568, -0.3955],\n",
       "         [ 1.4143,  0.3240,  0.8013,  1.2343,  1.4264],\n",
       "         [ 0.7334, -0.4415, -2.5634,  0.5602,  0.3290],\n",
       "         [-0.0218,  1.4542, -0.4096, -2.2609,  0.0848],\n",
       "         [-0.0903,  1.4839,  1.6288, -0.2279, -0.3493]], requires_grad=True)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 5\n",
    "embeds = nn.Embedding(len(vocabulary), embedding_dim)\n",
    "\n",
    "# show the embedding layer parameters (randomly initialized at the moment), i.e. the embedding matrix in which each row is the embedding vector for the corresponding word from the vocabulary\n",
    "list(embeds.parameters()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3550,  0.4291, -0.1219, -1.6771,  0.7565],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[ 0.3550,  0.4291, -0.1219, -1.6771,  0.7565],\n",
      "        [ 0.0562,  1.3598, -1.4467, -1.1859, -0.0695]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# to index into the embedding vector for a particular word, we need to use an index tensor of type torch.long\n",
    "index = word2ind[\"paris\"]\n",
    "index_tensor = torch.tensor(index, dtype=torch.long)\n",
    "paris_embed = embeds(index_tensor)\n",
    "print(paris_embed)\n",
    "\n",
    "# can also get embeddings for multiple word indices\n",
    "index_paris = word2ind[\"paris\"]\n",
    "index_ankara = word2ind[\"ankara\"]\n",
    "indices = [index_paris, index_ankara]\n",
    "indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "embeddings = embeds(indices_tensor)\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating batches of sentences using the `torch.util.data.DataLoader` class\n",
    "\n",
    "`DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)`\n",
    "\n",
    "The `collate_fn` is a user defined function that can be used to print stats about each batch and/or do some extra pre-processing to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _custom_collate_fn(batch, window_size, word2ind):\n",
    "    # split batch into sentences and labels\n",
    "    x, y = zip(*batch)\n",
    "\n",
    "    # pad the beginngin and end of sentences for window and convert to indices\n",
    "    x = [pad_sentence(s, window_size=window_size) for s in x]\n",
    "    x = [sentence_to_indices(s, word2ind) for s in x]\n",
    "\n",
    "    # convert each sequence of indices to pytorch tensor\n",
    "    x = [torch.LongTensor(indices) for indices in x]\n",
    "\n",
    "    # additional padding to make all sentences the same length using the pytorch nn.utils.rnn.pad_sequences function\n",
    "    pad_token_ix = word2ind[\"<pad>\"]\n",
    "    x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=pad_token_ix)\n",
    "\n",
    "    # also pad the labels and record original sentence length, i.e. length without padding\n",
    "    lengths = torch.LongTensor([len(label) for label in y])\n",
    "    y = [torch.LongTensor(label) for label in y]\n",
    "    y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
    "\n",
    "    return x_padded, y_padded, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch # 0\n",
      "Batched input: \n",
      "tensor([[ 0,  0, 22,  2,  6, 20, 15,  0,  0],\n",
      "        [ 0,  0, 19, 16, 12,  8,  4,  0,  0]])\n",
      "Batched labels: \n",
      "tensor([[0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 1]])\n",
      "Batched lengths: \n",
      "tensor([5, 5])\n",
      "\n",
      "Batch # 1\n",
      "Batched input: \n",
      "tensor([[ 0,  0, 10, 13, 11, 17,  0,  0,  0,  0],\n",
      "        [ 0,  0, 19,  5, 14, 21, 12,  3,  0,  0]])\n",
      "Batched labels: \n",
      "tensor([[0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 1]])\n",
      "Batched lengths: \n",
      "tensor([4, 6])\n",
      "\n",
      "Batch # 2\n",
      "Batched input: \n",
      "tensor([[ 0,  0,  9,  7,  8, 18,  0,  0]])\n",
      "Batched labels: \n",
      "tensor([[0, 0, 0, 1]])\n",
      "Batched lengths: \n",
      "tensor([4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataloader parameters\n",
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(_custom_collate_fn, window_size=window_size, word2ind=word2ind)\n",
    "\n",
    "# instantiate dataloader\n",
    "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "# show the batches\n",
    "counter = 0 \n",
    "for batched_x, batched_y, batched_lengths in loader:\n",
    "    print(f\"Batch # {counter}\")\n",
    "    print(\"Batched input: \")\n",
    "    print(batched_x)\n",
    "    print(\"Batched labels: \")\n",
    "    print(batched_y)\n",
    "    print(\"Batched lengths: \")\n",
    "    print(batched_lengths)\n",
    "    print()\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor: \n",
      "tensor([[ 0,  0,  9,  7,  8, 18,  0,  0]])\n",
      "\n",
      "Windows: \n",
      "tensor([[[ 0,  0,  9,  7,  8],\n",
      "         [ 0,  9,  7,  8, 18],\n",
      "         [ 9,  7,  8, 18,  0],\n",
      "         [ 7,  8, 18,  0,  0]]])\n"
     ]
    }
   ],
   "source": [
    "# creating all windows of an indexed sentence, i.e. all chunks of size 2*window_size +1\n",
    "print(f\"Original tensor: \")\n",
    "print(batched_x)\n",
    "print()\n",
    "\n",
    "# we will use the unfold function of pytorch to create the chunks\n",
    "chunks = batched_x.unfold(dimension=1, size=window_size*2+1, step=1)\n",
    "print(\"Windows: \")\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model as a custom module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordWindowClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, hyperparameters, vocab_size, pad_ix=0):\n",
    "        super(WordWindowClassifier, self).__init__()\n",
    "\n",
    "        # instance variables \n",
    "        self.vocab_size = vocab_size\n",
    "        self.pad_ix = pad_ix\n",
    "        self.window_size = hyperparameters[\"window_size\"]\n",
    "        self.embed_dim = hyperparameters[\"embed_dim\"]\n",
    "        self.hidden_dim = hyperparameters[\"hidden_dim\"]\n",
    "        self.freeze_embeddings = hyperparameters[\"freeze_embeddings\"]\n",
    "\n",
    "        # define layers of the neural network\n",
    "        self.embed_layer = nn.Embedding(self.vocab_size, self.embed_dim, padding_idx=self.pad_ix)\n",
    "        if self.freeze_embeddings:\n",
    "            # make the embeddings untrainable if the freeze_embeddings setting is on\n",
    "            self.embed_layer.weight.requires_grad=False\n",
    "\n",
    "        # input of hidden layer is the contatenated embedding vectors of all words in the full window \n",
    "        full_window_size = 2*self.window_size+1\n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(full_window_size * self.embed_dim, self.hidden_dim),\n",
    "            nn.Tanh()    \n",
    "        )\n",
    "\n",
    "        # output layer for binary classification\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    # implement forward pass\n",
    "    def forward(self, inputs): \n",
    "        # input is a batch of padded indexed sentences\n",
    "        # i.e. a tensor of shape (B, L)\n",
    "        # where B = batch_size and L = window padded sentence length\n",
    "\n",
    "        B, L = inputs.shape\n",
    "        S = 2*self.window_size+1\n",
    "\n",
    "        # we need to create token windows for the sentences\n",
    "        # converts input of shape (B, L) into shape (B, W, S)\n",
    "        # where W=number of windows and S=full window length\n",
    "        token_windows = inputs.unfold(1, S, 1)\n",
    "\n",
    "        # do a sanity check to make sure the token_windows tensor has the right shape\n",
    "        _, W , _ = token_windows.shape\n",
    "        assert token_windows.shape == (B, W, S), \"Error in token_windows!\" \n",
    "\n",
    "        # convert token indices to embedding vectors\n",
    "        # converts input of shape (B, W, S) into shape (B, W, S, D)\n",
    "        # where D=embedding dims\n",
    "        embedded_windows = self.embed_layer(token_windows)\n",
    "\n",
    "        # reshape so that all embedding vectors of words within a windo get concatenated\n",
    "        # converts input of shape (B, W, S, D) into shape (B, W, S*D)\n",
    "        embedded_windows = embedded_windows.view(B, W, -1)\n",
    "\n",
    "        # pass the inputs into hidden layer\n",
    "        # converts input of shape (B, W, S*D) into shape (B, W, H)\n",
    "        # where H=hidden dim\n",
    "        hidden_layer_output = self.hidden_layer(embedded_windows)\n",
    "\n",
    "        # pass to output layer \n",
    "        # converts input of shape (B, W, H) into shape (B, W, 1)\n",
    "        output = self.output_layer(hidden_layer_output)\n",
    "\n",
    "        # reshape output from (B, W, 1) into shape (B, W*1)\n",
    "        output = output.view(B,-1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep the data\n",
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(_custom_collate_fn, window_size=window_size, word2ind=word2ind)\n",
    "\n",
    "# instantiate dataloader\n",
    "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "# instantaite the model\n",
    "model_hyperparameters = {\n",
    "    \"batch_size\" : 4,\n",
    "    \"window_size\" : 2,\n",
    "    \"embed_dim\" : 25, \n",
    "    \"hidden_dim\" : 25,\n",
    "    \"freeze_embeddings\" : False\n",
    "}\n",
    "vocab_size = len(ind2word)\n",
    "model = WordWindowClassifier(model_hyperparameters, vocab_size)\n",
    "\n",
    "# define optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# define a custom loss function\n",
    "def loss_function(batch_outputs, batch_labels, batch_lengths):\n",
    "    # compute binary crossentropy loss for whole batch\n",
    "    bceloss = nn.BCELoss()\n",
    "    loss = bceloss(batch_outputs, batch_labels.float())\n",
    "\n",
    "    # rescale the loss by diving by total number of words in batch (original/unpadded sentences)\n",
    "    loss = loss / batch_lengths.sum().float()\n",
    "\n",
    "    return loss    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(loss_function, optimizer, model, loader):\n",
    "    \n",
    "    total_batch_loss = 0.0\n",
    "    for batch_inputs, batch_labels, batch_lengths in loader:\n",
    "        # clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        batch_outputs = model(batch_inputs)\n",
    "        # compute loss\n",
    "        loss = loss_function(batch_outputs, batch_labels, batch_lengths)\n",
    "        # backpropagate the grads\n",
    "        loss.backward()\n",
    "        # optimizer step to update the parameters\n",
    "        optimizer.step()\n",
    "        total_batch_loss += loss.item()\n",
    "\n",
    "    return total_batch_loss\n",
    "\n",
    "def train(loss_function, optimizer, model, loader, num_epochs=10000):\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = train_epoch(loss_function, optimizer, model, loader)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch# {epoch}, loss = {epoch_loss}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 0, loss = 0.3357775807380676\n",
      "Epoch# 100, loss = 0.2630639150738716\n",
      "Epoch# 200, loss = 0.2061525546014309\n",
      "Epoch# 300, loss = 0.15910308063030243\n",
      "Epoch# 400, loss = 0.12577852047979832\n",
      "Epoch# 500, loss = 0.10344464145600796\n",
      "Epoch# 600, loss = 0.0634592603892088\n",
      "Epoch# 700, loss = 0.05704061035066843\n",
      "Epoch# 800, loss = 0.045995863154530525\n",
      "Epoch# 900, loss = 0.03718583658337593\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "train(loss_function, optimizer, model, loader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions on test data using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = [\"She comes from Paris\"]\n",
    "test_sentences = [s.lower().split() for s in test_corpus]\n",
    "test_labels = [[0,0,0,1]]\n",
    "\n",
    "# create test dataset loader\n",
    "test_data = list(zip(test_sentences, test_labels))\n",
    "batch_size = 1\n",
    "shuffle = False\n",
    "window_size = 2\n",
    "collate_fn = partial(_custom_collate_fn, window_size=window_size, word2ind=word2ind)\n",
    "\n",
    "# instantiate dataloader\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: tensor([[0.0270, 0.0695, 0.0371, 0.9002]], grad_fn=<ViewBackward0>)\n",
      "Ground truth: tensor([[0, 0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "for test_instance, labels, _ in test_loader:\n",
    "    # compute predictions\n",
    "    outputs = model(test_instance)\n",
    "    print(f\"Predictions: {outputs}\")\n",
    "    print(f\"Ground truth: {labels}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
