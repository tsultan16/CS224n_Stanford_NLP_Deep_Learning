{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Language Models Continued..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 32033\n",
      "Shortest word: 2\n",
      "Longest word: 15\n"
     ]
    }
   ],
   "source": [
    "# load dataset and store the words in a list\n",
    "words = open('names.txt', 'r').read().split()\n",
    "\n",
    "print(f\"Total number of words: {len(words)}\")\n",
    "print(f\"Shortest word: {min([len(w) for w in words])}\")\n",
    "print(f\"Longest word: {max([len(w) for w in words])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We looked at the count based bi-gram language model and the equivalent simple neural network based bi-gram model which takes in a one-hot encoded character as input. Now we will try to build a language model using a neural network with one hidden layer which learns to predict the next character giving a sequence of multiple previous characters, so larger context window than bi-gram model. We will also represent each character of the vocabulary with a learned embedding vector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['*', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Character indices: {'*': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
     ]
    }
   ],
   "source": [
    "# now lets create an indexed vocabulary of characters\n",
    "pad_token = '*'\n",
    "vocab = sorted(set([pad_token] + list(\"\".join(words))))\n",
    "ctoi = {vocab[i]:i for i in range(len(vocab))}\n",
    "ctoi[pad_token] = 0 # special token for padding\n",
    "itoc = {i:vocab[i] for i in range(len(vocab))}\n",
    "print(f\"Vocabulary: {vocab}\")\n",
    "print(f\"Character indices: {ctoi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "# Now lets prepare the training dataset\n",
    "block_size = 3 # size of context window, i.e. number of previous characters in input sequence\n",
    "\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "    # word is padded on the left with block_size padding tokens and on the right with one padding token \n",
    "    w_ix = [ctoi[c] for c in w]\n",
    "    chars = [0]*block_size + w_ix + [0]\n",
    "    for i in range(len(chars)-block_size):\n",
    "        # context characters in window of size block_size\n",
    "        xc = chars[i:i+block_size]\n",
    "        # target character\n",
    "        yc = chars[i+block_size]\n",
    "        X.append(xc)\n",
    "        Y.append(yc)\n",
    "        #print(xc, \"--> \", yc)\n",
    "        \n",
    "# convert to pytorch tensors\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of embedding vectors, randomly initialized\n",
    "embedding_dims = 2\n",
    "C = torch.randn((len(vocab), embedding_dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 6])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert input character indices to embedding vectors and concatenate\n",
    "X_emb = C[X]\n",
    "shape = X_emb.shape\n",
    "X_emb = X_emb.view(-1, block_size*embedding_dims)\n",
    "X_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intiialize parameters of the hidden layer\n",
    "hidden_dims = 100\n",
    "W1 = torch.randn((block_size*embedding_dims, hidden_dims))\n",
    "b1 = torch.randn(hidden_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 100])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute hidden layer activations\n",
    "h = torch.tanh(X_emb @ W1 + b1)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize output layer parameters\n",
    "W2 = torch.randn((hidden_dims, len(vocab)))\n",
    "b2 = torch.randn(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute output logits\n",
    "logits = h @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.4521)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# compute average negative log-likelihood loss\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create train-dev-test (80-10-10) splits of the dataset\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        # word is padded on the left with block_size padding tokens and on the right with one padding token \n",
    "        '''\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '*':\n",
    "            ix = ctoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "        '''\n",
    "        w_ix = [ctoi[c] for c in w]\n",
    "        chars = [0]*block_size + w_ix + [0]\n",
    "        for i in range(len(chars)-block_size):\n",
    "            # context characters in window of size block_size\n",
    "            xc = chars[i:i+block_size]\n",
    "            # target character\n",
    "            yc = chars[i+block_size]\n",
    "            X.append(xc)\n",
    "            Y.append(yc)\n",
    "\n",
    "    # convert to pytorch tensors\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtrain, Ytrain = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xtest, Ytest = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: When initialiazing model parameters, we would like to ensure that the output logits during the beginning of training don't vary across a large range of values, otherwise we would get high loss values at the start. Therefore it's a good idea to initialize biases to zero and weigts to small random values close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of params: 12297\n"
     ]
    }
   ],
   "source": [
    "# create a pytorch generator\n",
    "g = torch.Generator().manual_seed(123)\n",
    "\n",
    "num_epochs = 50000\n",
    "lr = 0.01            # gradient descent learning rate\n",
    "batch_size = 32\n",
    "embedding_dims = 10\n",
    "hidden_dims = 200\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# initialize model parameters\n",
    "C = torch.randn((vocab_size, embedding_dims), generator=g)\n",
    "W1 = torch.randn((block_size*embedding_dims, hidden_dims), generator=g) * 0.01\n",
    "b1 = torch.randn(hidden_dims, generator=g) * 0\n",
    "W2 = torch.randn((hidden_dims, vocab_size), generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0\n",
    "\n",
    "# parameters for batch normalization\n",
    "bngain = torch.ones((1,hidden_dims))\n",
    "bnbias = torch.zeros((1,hidden_dims))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "    p.requires_grad=True\n",
    "\n",
    "print(f\"Total number of params: {sum(p.nelement() for p in parameters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs=1000, lr=0.01):\n",
    "    print(\"Training...\")\n",
    "    for i in range(num_epochs):\n",
    "        # randomly select a batch of input instances\n",
    "        batch_ix = torch.randint(0, Xtrain.shape[0], (batch_size,), generator=g)\n",
    "        X_batch, Y_batch = Xtrain[batch_ix], Ytrain[batch_ix] \n",
    "\n",
    "        # reset parameter gradients\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        \n",
    "        # forward pass\n",
    "        X_emb = C[X_batch].view(-1,block_size*embedding_dims)\n",
    "        h_linear = X_emb @ W1 + b1\n",
    "\n",
    "        # before passing into tanh activation function, we apply batch normalization\n",
    "        # batch norm involves normalizing the linear layer output neuron to have zero mean and unit variance \n",
    "        # over the entire batch of inputs. This is followed by scaling and shifting the values. The advantage \n",
    "        # of batch norm is that the linear layer outputs passed into the tanh will be mostly concentrated in \n",
    "        # the non-flat regions of the tanh function and so we avoid vanishing gradients during packprop and dead neurons \n",
    "\n",
    "        # normalize\n",
    "        h_linear = (h_linear - h_linear.mean(dim=0, keepdims=True)) / h_linear.std(dim=0, keepdims=True) \n",
    "        # scale and shift\n",
    "        h_linear = bngain * h_linear  + bnbias\n",
    "\n",
    "        h = torch.tanh(h_linear)\n",
    "        logits = h @ W2 + b2\n",
    "        loss = F.cross_entropy(logits, Y_batch)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # reduce learning rate by factor of 10 halfway through training\n",
    "        if(i//num_epochs == 2):\n",
    "            lr = 0.1 * lr\n",
    "\n",
    "        # update parameters via gradient descent\n",
    "        for p in parameters:\n",
    "            p.data -= lr * p.grad\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            print(f\"Epoch# {i}, Mini Batch Average Loss: {loss.item()}\")\n",
    "\n",
    "    return h_linear, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch# 0, Mini Batch Average Loss: 3.2673447132110596\n",
      "Epoch# 500, Mini Batch Average Loss: 2.4357049465179443\n",
      "Epoch# 1000, Mini Batch Average Loss: 2.7983624935150146\n",
      "Epoch# 1500, Mini Batch Average Loss: 2.187814950942993\n",
      "Epoch# 2000, Mini Batch Average Loss: 2.310910224914551\n",
      "Epoch# 2500, Mini Batch Average Loss: 2.4317586421966553\n",
      "Epoch# 3000, Mini Batch Average Loss: 2.3810248374938965\n",
      "Epoch# 3500, Mini Batch Average Loss: 2.1987428665161133\n",
      "Epoch# 4000, Mini Batch Average Loss: 2.608093738555908\n",
      "Epoch# 4500, Mini Batch Average Loss: 2.3451685905456543\n",
      "Epoch# 5000, Mini Batch Average Loss: 2.3413689136505127\n",
      "Epoch# 5500, Mini Batch Average Loss: 2.4834272861480713\n",
      "Epoch# 6000, Mini Batch Average Loss: 2.3467142581939697\n",
      "Epoch# 6500, Mini Batch Average Loss: 2.445197343826294\n",
      "Epoch# 7000, Mini Batch Average Loss: 2.217597484588623\n",
      "Epoch# 7500, Mini Batch Average Loss: 2.463128089904785\n",
      "Epoch# 8000, Mini Batch Average Loss: 2.399271249771118\n",
      "Epoch# 8500, Mini Batch Average Loss: 2.460223913192749\n",
      "Epoch# 9000, Mini Batch Average Loss: 1.820860743522644\n",
      "Epoch# 9500, Mini Batch Average Loss: 2.280102252960205\n",
      "Epoch# 10000, Mini Batch Average Loss: 1.898545265197754\n",
      "Epoch# 10500, Mini Batch Average Loss: 2.326618194580078\n",
      "Epoch# 11000, Mini Batch Average Loss: 2.097961902618408\n",
      "Epoch# 11500, Mini Batch Average Loss: 2.4493496417999268\n",
      "Epoch# 12000, Mini Batch Average Loss: 2.1304476261138916\n",
      "Epoch# 12500, Mini Batch Average Loss: 2.2284328937530518\n",
      "Epoch# 13000, Mini Batch Average Loss: 2.763286828994751\n",
      "Epoch# 13500, Mini Batch Average Loss: 2.012814998626709\n",
      "Epoch# 14000, Mini Batch Average Loss: 2.1241109371185303\n",
      "Epoch# 14500, Mini Batch Average Loss: 2.3798234462738037\n",
      "Epoch# 15000, Mini Batch Average Loss: 2.2536323070526123\n",
      "Epoch# 15500, Mini Batch Average Loss: 2.5365500450134277\n",
      "Epoch# 16000, Mini Batch Average Loss: 2.1946818828582764\n",
      "Epoch# 16500, Mini Batch Average Loss: 2.3164453506469727\n",
      "Epoch# 17000, Mini Batch Average Loss: 2.5365750789642334\n",
      "Epoch# 17500, Mini Batch Average Loss: 2.4271790981292725\n",
      "Epoch# 18000, Mini Batch Average Loss: 2.447392463684082\n",
      "Epoch# 18500, Mini Batch Average Loss: 1.9259625673294067\n",
      "Epoch# 19000, Mini Batch Average Loss: 1.9455368518829346\n",
      "Epoch# 19500, Mini Batch Average Loss: 2.139313220977783\n",
      "Epoch# 20000, Mini Batch Average Loss: 2.206266164779663\n",
      "Epoch# 20500, Mini Batch Average Loss: 2.3445053100585938\n",
      "Epoch# 21000, Mini Batch Average Loss: 2.2652535438537598\n",
      "Epoch# 21500, Mini Batch Average Loss: 2.035012722015381\n",
      "Epoch# 22000, Mini Batch Average Loss: 2.356048345565796\n",
      "Epoch# 22500, Mini Batch Average Loss: 2.087407112121582\n",
      "Epoch# 23000, Mini Batch Average Loss: 2.0505788326263428\n",
      "Epoch# 23500, Mini Batch Average Loss: 2.2119357585906982\n",
      "Epoch# 24000, Mini Batch Average Loss: 2.1504597663879395\n",
      "Epoch# 24500, Mini Batch Average Loss: 2.290473699569702\n",
      "Epoch# 25000, Mini Batch Average Loss: 2.376298427581787\n",
      "Epoch# 25500, Mini Batch Average Loss: 2.330845832824707\n",
      "Epoch# 26000, Mini Batch Average Loss: 2.1452441215515137\n",
      "Epoch# 26500, Mini Batch Average Loss: 1.7995175123214722\n",
      "Epoch# 27000, Mini Batch Average Loss: 2.5702011585235596\n",
      "Epoch# 27500, Mini Batch Average Loss: 2.5594873428344727\n",
      "Epoch# 28000, Mini Batch Average Loss: 2.431025981903076\n",
      "Epoch# 28500, Mini Batch Average Loss: 2.3535683155059814\n",
      "Epoch# 29000, Mini Batch Average Loss: 1.9145158529281616\n",
      "Epoch# 29500, Mini Batch Average Loss: 2.330376625061035\n",
      "Epoch# 30000, Mini Batch Average Loss: 2.036907434463501\n",
      "Epoch# 30500, Mini Batch Average Loss: 2.4890968799591064\n",
      "Epoch# 31000, Mini Batch Average Loss: 1.9902527332305908\n",
      "Epoch# 31500, Mini Batch Average Loss: 2.389465570449829\n",
      "Epoch# 32000, Mini Batch Average Loss: 2.0606486797332764\n",
      "Epoch# 32500, Mini Batch Average Loss: 2.4467976093292236\n",
      "Epoch# 33000, Mini Batch Average Loss: 2.771735429763794\n",
      "Epoch# 33500, Mini Batch Average Loss: 2.1466333866119385\n",
      "Epoch# 34000, Mini Batch Average Loss: 2.20807147026062\n",
      "Epoch# 34500, Mini Batch Average Loss: 2.0139336585998535\n",
      "Epoch# 35000, Mini Batch Average Loss: 2.0783443450927734\n",
      "Epoch# 35500, Mini Batch Average Loss: 1.9151602983474731\n",
      "Epoch# 36000, Mini Batch Average Loss: 2.3387980461120605\n",
      "Epoch# 36500, Mini Batch Average Loss: 2.1046619415283203\n",
      "Epoch# 37000, Mini Batch Average Loss: 2.174705982208252\n",
      "Epoch# 37500, Mini Batch Average Loss: 2.3880295753479004\n",
      "Epoch# 38000, Mini Batch Average Loss: 2.6843042373657227\n",
      "Epoch# 38500, Mini Batch Average Loss: 2.573622465133667\n",
      "Epoch# 39000, Mini Batch Average Loss: 2.118943214416504\n",
      "Epoch# 39500, Mini Batch Average Loss: 1.988608956336975\n",
      "Epoch# 40000, Mini Batch Average Loss: 2.402569055557251\n",
      "Epoch# 40500, Mini Batch Average Loss: 2.0320589542388916\n",
      "Epoch# 41000, Mini Batch Average Loss: 2.2966578006744385\n",
      "Epoch# 41500, Mini Batch Average Loss: 2.384852647781372\n",
      "Epoch# 42000, Mini Batch Average Loss: 1.999105453491211\n",
      "Epoch# 42500, Mini Batch Average Loss: 2.293714761734009\n",
      "Epoch# 43000, Mini Batch Average Loss: 1.9171160459518433\n",
      "Epoch# 43500, Mini Batch Average Loss: 2.5233306884765625\n",
      "Epoch# 44000, Mini Batch Average Loss: 2.457444667816162\n",
      "Epoch# 44500, Mini Batch Average Loss: 2.0841166973114014\n",
      "Epoch# 45000, Mini Batch Average Loss: 2.44836688041687\n",
      "Epoch# 45500, Mini Batch Average Loss: 2.16849422454834\n",
      "Epoch# 46000, Mini Batch Average Loss: 2.12577486038208\n",
      "Epoch# 46500, Mini Batch Average Loss: 2.040287971496582\n",
      "Epoch# 47000, Mini Batch Average Loss: 2.3204076290130615\n",
      "Epoch# 47500, Mini Batch Average Loss: 2.59822154045105\n",
      "Epoch# 48000, Mini Batch Average Loss: 1.9734569787979126\n",
      "Epoch# 48500, Mini Batch Average Loss: 2.3446455001831055\n",
      "Epoch# 49000, Mini Batch Average Loss: 2.1726245880126953\n",
      "Epoch# 49500, Mini Batch Average Loss: 1.9161763191223145\n",
      "Epoch# 50000, Mini Batch Average Loss: 2.095273733139038\n",
      "Epoch# 50500, Mini Batch Average Loss: 3.025968074798584\n",
      "Epoch# 51000, Mini Batch Average Loss: 2.145477294921875\n",
      "Epoch# 51500, Mini Batch Average Loss: 2.2172772884368896\n",
      "Epoch# 52000, Mini Batch Average Loss: 1.9771959781646729\n",
      "Epoch# 52500, Mini Batch Average Loss: 2.0578153133392334\n",
      "Epoch# 53000, Mini Batch Average Loss: 1.7809016704559326\n",
      "Epoch# 53500, Mini Batch Average Loss: 1.9362126588821411\n",
      "Epoch# 54000, Mini Batch Average Loss: 1.976100206375122\n",
      "Epoch# 54500, Mini Batch Average Loss: 2.263932704925537\n",
      "Epoch# 55000, Mini Batch Average Loss: 2.044851779937744\n",
      "Epoch# 55500, Mini Batch Average Loss: 2.123115062713623\n",
      "Epoch# 56000, Mini Batch Average Loss: 1.969902753829956\n",
      "Epoch# 56500, Mini Batch Average Loss: 2.063039541244507\n",
      "Epoch# 57000, Mini Batch Average Loss: 2.1922121047973633\n",
      "Epoch# 57500, Mini Batch Average Loss: 2.355942487716675\n",
      "Epoch# 58000, Mini Batch Average Loss: 2.140256404876709\n",
      "Epoch# 58500, Mini Batch Average Loss: 1.9704324007034302\n",
      "Epoch# 59000, Mini Batch Average Loss: 2.1435983180999756\n",
      "Epoch# 59500, Mini Batch Average Loss: 2.188136339187622\n",
      "Epoch# 60000, Mini Batch Average Loss: 2.229191303253174\n",
      "Epoch# 60500, Mini Batch Average Loss: 2.1966640949249268\n",
      "Epoch# 61000, Mini Batch Average Loss: 2.071612596511841\n",
      "Epoch# 61500, Mini Batch Average Loss: 2.0449371337890625\n",
      "Epoch# 62000, Mini Batch Average Loss: 2.2464065551757812\n",
      "Epoch# 62500, Mini Batch Average Loss: 2.1942138671875\n",
      "Epoch# 63000, Mini Batch Average Loss: 1.9610127210617065\n",
      "Epoch# 63500, Mini Batch Average Loss: 2.394407272338867\n",
      "Epoch# 64000, Mini Batch Average Loss: 2.371664524078369\n",
      "Epoch# 64500, Mini Batch Average Loss: 2.267355442047119\n",
      "Epoch# 65000, Mini Batch Average Loss: 2.2041356563568115\n",
      "Epoch# 65500, Mini Batch Average Loss: 2.3479321002960205\n",
      "Epoch# 66000, Mini Batch Average Loss: 1.7934788465499878\n",
      "Epoch# 66500, Mini Batch Average Loss: 2.2345306873321533\n",
      "Epoch# 67000, Mini Batch Average Loss: 2.2767741680145264\n",
      "Epoch# 67500, Mini Batch Average Loss: 2.1877894401550293\n",
      "Epoch# 68000, Mini Batch Average Loss: 2.4257540702819824\n",
      "Epoch# 68500, Mini Batch Average Loss: 2.303097724914551\n",
      "Epoch# 69000, Mini Batch Average Loss: 2.438926935195923\n",
      "Epoch# 69500, Mini Batch Average Loss: 2.3870253562927246\n",
      "Epoch# 70000, Mini Batch Average Loss: 2.0854685306549072\n",
      "Epoch# 70500, Mini Batch Average Loss: 2.3709914684295654\n",
      "Epoch# 71000, Mini Batch Average Loss: 2.03507137298584\n",
      "Epoch# 71500, Mini Batch Average Loss: 2.2678985595703125\n",
      "Epoch# 72000, Mini Batch Average Loss: 2.0115652084350586\n",
      "Epoch# 72500, Mini Batch Average Loss: 2.164292573928833\n",
      "Epoch# 73000, Mini Batch Average Loss: 2.2420668601989746\n",
      "Epoch# 73500, Mini Batch Average Loss: 1.8840999603271484\n",
      "Epoch# 74000, Mini Batch Average Loss: 2.1966311931610107\n",
      "Epoch# 74500, Mini Batch Average Loss: 2.2052876949310303\n",
      "Epoch# 75000, Mini Batch Average Loss: 2.320763349533081\n",
      "Epoch# 75500, Mini Batch Average Loss: 1.901148796081543\n",
      "Epoch# 76000, Mini Batch Average Loss: 2.3939592838287354\n",
      "Epoch# 76500, Mini Batch Average Loss: 1.9938546419143677\n",
      "Epoch# 77000, Mini Batch Average Loss: 2.4021079540252686\n",
      "Epoch# 77500, Mini Batch Average Loss: 1.9648340940475464\n",
      "Epoch# 78000, Mini Batch Average Loss: 2.5479323863983154\n",
      "Epoch# 78500, Mini Batch Average Loss: 2.191138505935669\n",
      "Epoch# 79000, Mini Batch Average Loss: 2.090027332305908\n",
      "Epoch# 79500, Mini Batch Average Loss: 2.2226951122283936\n",
      "Epoch# 80000, Mini Batch Average Loss: 2.33249831199646\n",
      "Epoch# 80500, Mini Batch Average Loss: 2.0840702056884766\n",
      "Epoch# 81000, Mini Batch Average Loss: 2.241626024246216\n",
      "Epoch# 81500, Mini Batch Average Loss: 2.282097578048706\n",
      "Epoch# 82000, Mini Batch Average Loss: 2.2382631301879883\n",
      "Epoch# 82500, Mini Batch Average Loss: 1.9751712083816528\n",
      "Epoch# 83000, Mini Batch Average Loss: 2.524292230606079\n",
      "Epoch# 83500, Mini Batch Average Loss: 1.8472987413406372\n",
      "Epoch# 84000, Mini Batch Average Loss: 1.9289907217025757\n",
      "Epoch# 84500, Mini Batch Average Loss: 1.8181533813476562\n",
      "Epoch# 85000, Mini Batch Average Loss: 2.3424274921417236\n",
      "Epoch# 85500, Mini Batch Average Loss: 2.3138391971588135\n",
      "Epoch# 86000, Mini Batch Average Loss: 2.4223642349243164\n",
      "Epoch# 86500, Mini Batch Average Loss: 2.3295767307281494\n",
      "Epoch# 87000, Mini Batch Average Loss: 2.3157503604888916\n",
      "Epoch# 87500, Mini Batch Average Loss: 2.005164384841919\n",
      "Epoch# 88000, Mini Batch Average Loss: 1.8512760400772095\n",
      "Epoch# 88500, Mini Batch Average Loss: 2.240888833999634\n",
      "Epoch# 89000, Mini Batch Average Loss: 2.311643600463867\n",
      "Epoch# 89500, Mini Batch Average Loss: 2.2680888175964355\n",
      "Epoch# 90000, Mini Batch Average Loss: 2.413957357406616\n",
      "Epoch# 90500, Mini Batch Average Loss: 1.9767051935195923\n",
      "Epoch# 91000, Mini Batch Average Loss: 2.578352928161621\n",
      "Epoch# 91500, Mini Batch Average Loss: 2.4457483291625977\n",
      "Epoch# 92000, Mini Batch Average Loss: 2.356520891189575\n",
      "Epoch# 92500, Mini Batch Average Loss: 2.0585334300994873\n",
      "Epoch# 93000, Mini Batch Average Loss: 1.9340561628341675\n",
      "Epoch# 93500, Mini Batch Average Loss: 2.384174346923828\n",
      "Epoch# 94000, Mini Batch Average Loss: 2.2826123237609863\n",
      "Epoch# 94500, Mini Batch Average Loss: 1.9012994766235352\n",
      "Epoch# 95000, Mini Batch Average Loss: 1.720388650894165\n",
      "Epoch# 95500, Mini Batch Average Loss: 2.1865041255950928\n",
      "Epoch# 96000, Mini Batch Average Loss: 2.1273229122161865\n",
      "Epoch# 96500, Mini Batch Average Loss: 2.5149447917938232\n",
      "Epoch# 97000, Mini Batch Average Loss: 1.789088249206543\n",
      "Epoch# 97500, Mini Batch Average Loss: 2.3034451007843018\n",
      "Epoch# 98000, Mini Batch Average Loss: 2.3939199447631836\n",
      "Epoch# 98500, Mini Batch Average Loss: 1.841269612312317\n",
      "Epoch# 99000, Mini Batch Average Loss: 2.115764856338501\n",
      "Epoch# 99500, Mini Batch Average Loss: 2.305835008621216\n",
      "Epoch# 100000, Mini Batch Average Loss: 2.320302724838257\n",
      "Epoch# 100500, Mini Batch Average Loss: 2.343972682952881\n",
      "Epoch# 101000, Mini Batch Average Loss: 2.0741500854492188\n",
      "Epoch# 101500, Mini Batch Average Loss: 2.1199088096618652\n",
      "Epoch# 102000, Mini Batch Average Loss: 2.3253653049468994\n",
      "Epoch# 102500, Mini Batch Average Loss: 1.844834566116333\n",
      "Epoch# 103000, Mini Batch Average Loss: 2.072648763656616\n",
      "Epoch# 103500, Mini Batch Average Loss: 1.695428729057312\n",
      "Epoch# 104000, Mini Batch Average Loss: 2.2502825260162354\n",
      "Epoch# 104500, Mini Batch Average Loss: 2.2126517295837402\n",
      "Epoch# 105000, Mini Batch Average Loss: 1.890592336654663\n",
      "Epoch# 105500, Mini Batch Average Loss: 1.7755212783813477\n",
      "Epoch# 106000, Mini Batch Average Loss: 2.24625825881958\n",
      "Epoch# 106500, Mini Batch Average Loss: 1.8950228691101074\n",
      "Epoch# 107000, Mini Batch Average Loss: 2.2178688049316406\n",
      "Epoch# 107500, Mini Batch Average Loss: 2.000838279724121\n",
      "Epoch# 108000, Mini Batch Average Loss: 2.299436569213867\n",
      "Epoch# 108500, Mini Batch Average Loss: 1.9467889070510864\n",
      "Epoch# 109000, Mini Batch Average Loss: 2.1385397911071777\n",
      "Epoch# 109500, Mini Batch Average Loss: 2.259350538253784\n",
      "Epoch# 110000, Mini Batch Average Loss: 1.8796621561050415\n",
      "Epoch# 110500, Mini Batch Average Loss: 1.8494025468826294\n",
      "Epoch# 111000, Mini Batch Average Loss: 2.069317579269409\n",
      "Epoch# 111500, Mini Batch Average Loss: 1.8735023736953735\n",
      "Epoch# 112000, Mini Batch Average Loss: 2.046302556991577\n",
      "Epoch# 112500, Mini Batch Average Loss: 2.106600046157837\n",
      "Epoch# 113000, Mini Batch Average Loss: 2.2860498428344727\n",
      "Epoch# 113500, Mini Batch Average Loss: 2.4603095054626465\n",
      "Epoch# 114000, Mini Batch Average Loss: 2.1618776321411133\n",
      "Epoch# 114500, Mini Batch Average Loss: 1.9674491882324219\n",
      "Epoch# 115000, Mini Batch Average Loss: 2.1818113327026367\n",
      "Epoch# 115500, Mini Batch Average Loss: 1.781766414642334\n",
      "Epoch# 116000, Mini Batch Average Loss: 2.2531721591949463\n",
      "Epoch# 116500, Mini Batch Average Loss: 2.225658893585205\n",
      "Epoch# 117000, Mini Batch Average Loss: 2.128023147583008\n",
      "Epoch# 117500, Mini Batch Average Loss: 2.243839979171753\n",
      "Epoch# 118000, Mini Batch Average Loss: 1.9209495782852173\n",
      "Epoch# 118500, Mini Batch Average Loss: 2.2832887172698975\n",
      "Epoch# 119000, Mini Batch Average Loss: 1.8440146446228027\n",
      "Epoch# 119500, Mini Batch Average Loss: 2.1024603843688965\n",
      "Epoch# 120000, Mini Batch Average Loss: 2.1492958068847656\n",
      "Epoch# 120500, Mini Batch Average Loss: 2.1409523487091064\n",
      "Epoch# 121000, Mini Batch Average Loss: 2.0480916500091553\n",
      "Epoch# 121500, Mini Batch Average Loss: 1.8816131353378296\n",
      "Epoch# 122000, Mini Batch Average Loss: 2.162227153778076\n",
      "Epoch# 122500, Mini Batch Average Loss: 1.9691718816757202\n",
      "Epoch# 123000, Mini Batch Average Loss: 2.354139566421509\n",
      "Epoch# 123500, Mini Batch Average Loss: 2.21317195892334\n",
      "Epoch# 124000, Mini Batch Average Loss: 2.212564468383789\n",
      "Epoch# 124500, Mini Batch Average Loss: 1.8688087463378906\n",
      "Epoch# 125000, Mini Batch Average Loss: 2.4648807048797607\n",
      "Epoch# 125500, Mini Batch Average Loss: 2.0563063621520996\n",
      "Epoch# 126000, Mini Batch Average Loss: 1.9976232051849365\n",
      "Epoch# 126500, Mini Batch Average Loss: 1.9691156148910522\n",
      "Epoch# 127000, Mini Batch Average Loss: 1.9339481592178345\n",
      "Epoch# 127500, Mini Batch Average Loss: 2.1021955013275146\n",
      "Epoch# 128000, Mini Batch Average Loss: 2.1859617233276367\n",
      "Epoch# 128500, Mini Batch Average Loss: 2.4896130561828613\n",
      "Epoch# 129000, Mini Batch Average Loss: 2.2962310314178467\n",
      "Epoch# 129500, Mini Batch Average Loss: 1.9737954139709473\n",
      "Epoch# 130000, Mini Batch Average Loss: 1.7297945022583008\n",
      "Epoch# 130500, Mini Batch Average Loss: 2.074758529663086\n",
      "Epoch# 131000, Mini Batch Average Loss: 2.3639402389526367\n",
      "Epoch# 131500, Mini Batch Average Loss: 2.288280725479126\n",
      "Epoch# 132000, Mini Batch Average Loss: 2.0379998683929443\n",
      "Epoch# 132500, Mini Batch Average Loss: 2.4474658966064453\n",
      "Epoch# 133000, Mini Batch Average Loss: 2.0853028297424316\n",
      "Epoch# 133500, Mini Batch Average Loss: 1.8046308755874634\n",
      "Epoch# 134000, Mini Batch Average Loss: 2.0679826736450195\n",
      "Epoch# 134500, Mini Batch Average Loss: 2.152695417404175\n",
      "Epoch# 135000, Mini Batch Average Loss: 1.976422667503357\n",
      "Epoch# 135500, Mini Batch Average Loss: 2.0119330883026123\n",
      "Epoch# 136000, Mini Batch Average Loss: 1.9150034189224243\n",
      "Epoch# 136500, Mini Batch Average Loss: 2.542933702468872\n",
      "Epoch# 137000, Mini Batch Average Loss: 1.8700909614562988\n",
      "Epoch# 137500, Mini Batch Average Loss: 2.01076340675354\n",
      "Epoch# 138000, Mini Batch Average Loss: 2.1120071411132812\n",
      "Epoch# 138500, Mini Batch Average Loss: 2.2997820377349854\n",
      "Epoch# 139000, Mini Batch Average Loss: 1.8485077619552612\n",
      "Epoch# 139500, Mini Batch Average Loss: 2.5073018074035645\n",
      "Epoch# 140000, Mini Batch Average Loss: 2.0764973163604736\n",
      "Epoch# 140500, Mini Batch Average Loss: 1.766471266746521\n",
      "Epoch# 141000, Mini Batch Average Loss: 1.64950692653656\n",
      "Epoch# 141500, Mini Batch Average Loss: 2.146299362182617\n",
      "Epoch# 142000, Mini Batch Average Loss: 1.9999829530715942\n",
      "Epoch# 142500, Mini Batch Average Loss: 2.272768259048462\n",
      "Epoch# 143000, Mini Batch Average Loss: 2.31788969039917\n",
      "Epoch# 143500, Mini Batch Average Loss: 2.10125994682312\n",
      "Epoch# 144000, Mini Batch Average Loss: 1.693337082862854\n",
      "Epoch# 144500, Mini Batch Average Loss: 2.2220895290374756\n",
      "Epoch# 145000, Mini Batch Average Loss: 2.1674890518188477\n",
      "Epoch# 145500, Mini Batch Average Loss: 2.1637582778930664\n",
      "Epoch# 146000, Mini Batch Average Loss: 2.1313111782073975\n",
      "Epoch# 146500, Mini Batch Average Loss: 2.212909698486328\n",
      "Epoch# 147000, Mini Batch Average Loss: 1.9608536958694458\n",
      "Epoch# 147500, Mini Batch Average Loss: 2.572636127471924\n",
      "Epoch# 148000, Mini Batch Average Loss: 2.2396249771118164\n",
      "Epoch# 148500, Mini Batch Average Loss: 2.1815977096557617\n",
      "Epoch# 149000, Mini Batch Average Loss: 2.1518185138702393\n",
      "Epoch# 149500, Mini Batch Average Loss: 2.3852860927581787\n",
      "Epoch# 150000, Mini Batch Average Loss: 1.9860361814498901\n",
      "Epoch# 150500, Mini Batch Average Loss: 2.0827395915985107\n",
      "Epoch# 151000, Mini Batch Average Loss: 2.2376203536987305\n",
      "Epoch# 151500, Mini Batch Average Loss: 2.2321739196777344\n",
      "Epoch# 152000, Mini Batch Average Loss: 2.036085367202759\n",
      "Epoch# 152500, Mini Batch Average Loss: 1.976812720298767\n",
      "Epoch# 153000, Mini Batch Average Loss: 2.17276668548584\n",
      "Epoch# 153500, Mini Batch Average Loss: 1.9557280540466309\n",
      "Epoch# 154000, Mini Batch Average Loss: 1.8287440538406372\n",
      "Epoch# 154500, Mini Batch Average Loss: 2.379739284515381\n",
      "Epoch# 155000, Mini Batch Average Loss: 2.1616387367248535\n",
      "Epoch# 155500, Mini Batch Average Loss: 2.1836435794830322\n",
      "Epoch# 156000, Mini Batch Average Loss: 2.325721502304077\n",
      "Epoch# 156500, Mini Batch Average Loss: 2.1218738555908203\n",
      "Epoch# 157000, Mini Batch Average Loss: 2.1070597171783447\n",
      "Epoch# 157500, Mini Batch Average Loss: 2.1984336376190186\n",
      "Epoch# 158000, Mini Batch Average Loss: 2.3461785316467285\n",
      "Epoch# 158500, Mini Batch Average Loss: 2.199960231781006\n",
      "Epoch# 159000, Mini Batch Average Loss: 2.2967729568481445\n",
      "Epoch# 159500, Mini Batch Average Loss: 1.796546220779419\n",
      "Epoch# 160000, Mini Batch Average Loss: 2.348174810409546\n",
      "Epoch# 160500, Mini Batch Average Loss: 1.9941895008087158\n",
      "Epoch# 161000, Mini Batch Average Loss: 2.1765668392181396\n",
      "Epoch# 161500, Mini Batch Average Loss: 2.221405267715454\n",
      "Epoch# 162000, Mini Batch Average Loss: 1.9932984113693237\n",
      "Epoch# 162500, Mini Batch Average Loss: 2.3363208770751953\n",
      "Epoch# 163000, Mini Batch Average Loss: 2.0760018825531006\n",
      "Epoch# 163500, Mini Batch Average Loss: 2.1134791374206543\n",
      "Epoch# 164000, Mini Batch Average Loss: 2.032414674758911\n",
      "Epoch# 164500, Mini Batch Average Loss: 2.100130796432495\n",
      "Epoch# 165000, Mini Batch Average Loss: 1.8358312845230103\n",
      "Epoch# 165500, Mini Batch Average Loss: 2.3184542655944824\n",
      "Epoch# 166000, Mini Batch Average Loss: 2.127671480178833\n",
      "Epoch# 166500, Mini Batch Average Loss: 1.881137728691101\n",
      "Epoch# 167000, Mini Batch Average Loss: 2.1695518493652344\n",
      "Epoch# 167500, Mini Batch Average Loss: 2.1741228103637695\n",
      "Epoch# 168000, Mini Batch Average Loss: 2.180283308029175\n",
      "Epoch# 168500, Mini Batch Average Loss: 2.4075515270233154\n",
      "Epoch# 169000, Mini Batch Average Loss: 2.4784159660339355\n",
      "Epoch# 169500, Mini Batch Average Loss: 2.003697156906128\n",
      "Epoch# 170000, Mini Batch Average Loss: 1.938650369644165\n",
      "Epoch# 170500, Mini Batch Average Loss: 1.953302264213562\n",
      "Epoch# 171000, Mini Batch Average Loss: 2.287381649017334\n",
      "Epoch# 171500, Mini Batch Average Loss: 1.5919196605682373\n",
      "Epoch# 172000, Mini Batch Average Loss: 1.9878345727920532\n",
      "Epoch# 172500, Mini Batch Average Loss: 2.718987226486206\n",
      "Epoch# 173000, Mini Batch Average Loss: 1.9999841451644897\n",
      "Epoch# 173500, Mini Batch Average Loss: 2.0766165256500244\n",
      "Epoch# 174000, Mini Batch Average Loss: 2.0439014434814453\n",
      "Epoch# 174500, Mini Batch Average Loss: 2.336329698562622\n",
      "Epoch# 175000, Mini Batch Average Loss: 1.8908257484436035\n",
      "Epoch# 175500, Mini Batch Average Loss: 2.123680830001831\n",
      "Epoch# 176000, Mini Batch Average Loss: 2.0184214115142822\n",
      "Epoch# 176500, Mini Batch Average Loss: 2.4295566082000732\n",
      "Epoch# 177000, Mini Batch Average Loss: 1.8902639150619507\n",
      "Epoch# 177500, Mini Batch Average Loss: 2.3958003520965576\n",
      "Epoch# 178000, Mini Batch Average Loss: 2.0782182216644287\n",
      "Epoch# 178500, Mini Batch Average Loss: 1.7576725482940674\n",
      "Epoch# 179000, Mini Batch Average Loss: 1.8360801935195923\n",
      "Epoch# 179500, Mini Batch Average Loss: 2.1648433208465576\n",
      "Epoch# 180000, Mini Batch Average Loss: 2.044675350189209\n",
      "Epoch# 180500, Mini Batch Average Loss: 2.498187780380249\n",
      "Epoch# 181000, Mini Batch Average Loss: 1.9930872917175293\n",
      "Epoch# 181500, Mini Batch Average Loss: 1.9968829154968262\n",
      "Epoch# 182000, Mini Batch Average Loss: 2.2457468509674072\n",
      "Epoch# 182500, Mini Batch Average Loss: 2.2969300746917725\n",
      "Epoch# 183000, Mini Batch Average Loss: 2.0812346935272217\n",
      "Epoch# 183500, Mini Batch Average Loss: 2.040541172027588\n",
      "Epoch# 184000, Mini Batch Average Loss: 1.8819040060043335\n",
      "Epoch# 184500, Mini Batch Average Loss: 2.1976516246795654\n",
      "Epoch# 185000, Mini Batch Average Loss: 1.541074514389038\n",
      "Epoch# 185500, Mini Batch Average Loss: 1.9010175466537476\n",
      "Epoch# 186000, Mini Batch Average Loss: 2.0673978328704834\n",
      "Epoch# 186500, Mini Batch Average Loss: 1.8037073612213135\n",
      "Epoch# 187000, Mini Batch Average Loss: 1.9889533519744873\n",
      "Epoch# 187500, Mini Batch Average Loss: 2.2815425395965576\n",
      "Epoch# 188000, Mini Batch Average Loss: 2.4411096572875977\n",
      "Epoch# 188500, Mini Batch Average Loss: 2.4431936740875244\n",
      "Epoch# 189000, Mini Batch Average Loss: 2.280113697052002\n",
      "Epoch# 189500, Mini Batch Average Loss: 2.2140910625457764\n",
      "Epoch# 190000, Mini Batch Average Loss: 2.1140570640563965\n",
      "Epoch# 190500, Mini Batch Average Loss: 2.2859342098236084\n",
      "Epoch# 191000, Mini Batch Average Loss: 2.200134754180908\n",
      "Epoch# 191500, Mini Batch Average Loss: 2.475407361984253\n",
      "Epoch# 192000, Mini Batch Average Loss: 2.085099697113037\n",
      "Epoch# 192500, Mini Batch Average Loss: 2.2264347076416016\n",
      "Epoch# 193000, Mini Batch Average Loss: 1.8777225017547607\n",
      "Epoch# 193500, Mini Batch Average Loss: 2.294034004211426\n",
      "Epoch# 194000, Mini Batch Average Loss: 1.9940879344940186\n",
      "Epoch# 194500, Mini Batch Average Loss: 1.7963749170303345\n",
      "Epoch# 195000, Mini Batch Average Loss: 2.257227659225464\n",
      "Epoch# 195500, Mini Batch Average Loss: 2.335221529006958\n",
      "Epoch# 196000, Mini Batch Average Loss: 2.495645761489868\n",
      "Epoch# 196500, Mini Batch Average Loss: 2.3250274658203125\n",
      "Epoch# 197000, Mini Batch Average Loss: 2.048588991165161\n",
      "Epoch# 197500, Mini Batch Average Loss: 2.0756969451904297\n",
      "Epoch# 198000, Mini Batch Average Loss: 2.209740400314331\n",
      "Epoch# 198500, Mini Batch Average Loss: 1.9609745740890503\n",
      "Epoch# 199000, Mini Batch Average Loss: 2.4472670555114746\n",
      "Epoch# 199500, Mini Batch Average Loss: 2.11655855178833\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "lr = 0.01\n",
    "num_epochs = 200000\n",
    "h_linear, h = train(num_epochs=num_epochs, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've used batch normalization during training, we need to calibrate the batch normalization parameters so that they can be used correctly during inference. The calibration involves estimating the mean and standard deviations of the hidden linear layer neurons and using these values during inference for normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch norm calibration using training set\n",
    "with torch.no_grad():\n",
    "    X_emb = C[Xtrain].view(-1,block_size*embedding_dims)\n",
    "    h_linear = X_emb @ W1 + b1\n",
    "    # estimate mean and std over the entire training set\n",
    "    bnmean = h_linear.mean(dim=0, keepdims=True)\n",
    "    bnstd = h_linear.std(dim=0, keepdims=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # disable gradient tracking\n",
    "def split_loss(split='train'):\n",
    "    X, Y = {'train' : (Xtrain, Ytrain), 'val': (Xdev, Ydev), 'test': (Xtest, Ytest)}[split]\n",
    "    X_emb = C[X].view(-1,block_size*embedding_dims)\n",
    "    h_linear = X_emb @ W1 + b1\n",
    "    h_linear = (h_linear - bnmean) / bnstd \n",
    "    h_linear = bngain * h_linear  + bnbias\n",
    "    h = torch.tanh(h_linear)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(f\"{split} loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 2.095407485961914\n",
      "val loss: 2.12823748588562\n",
      "test loss: 2.1415131092071533\n"
     ]
    }
   ],
   "source": [
    "split_loss('train')\n",
    "split_loss('val')\n",
    "split_loss('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # disable gradient tracking\n",
    "def generate_word_nn(start_char=None):\n",
    "\n",
    "    generated_word = []\n",
    "    end_ix = ctoi[pad_token]\n",
    "\n",
    "    # start with the special token\n",
    "    if start_char is None:\n",
    "        context = [end_ix]*block_size\n",
    "    else:\n",
    "        assert start_char in vocab, \"Error! Start character must be a letter from the English alphabet.\"\n",
    "        context = [end_ix]*(block_size-1) + [ctoi[start_char]]\n",
    "        generated_word.append(start_char)\n",
    "\n",
    "    # generate characters one by one by sampling from the probability distribution p(c2|c1)\n",
    "    while True:\n",
    "        # get probabilities from the neural network\n",
    "        X_emb = C[torch.tensor([context])].view(1,-1)\n",
    "\n",
    "        h_linear = X_emb @ W1 + b1\n",
    "        h_linear = (h_linear - bnmean) / bnstd \n",
    "        h_linear = bngain * h_linear  + bnbias\n",
    "        h = torch.tanh(h_linear)\n",
    "        logits = h @ W2 + b2\n",
    "        p = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # sample the next character \n",
    "        cnext = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "\n",
    "        # stop if we generate the special token\n",
    "        if cnext==end_ix:\n",
    "            break\n",
    "        \n",
    "        context = context[1:] + [cnext]\n",
    "\n",
    "        generated_word.append(itoc[cnext])\n",
    "\n",
    "    return ''.join(generated_word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "khali\n",
      "ara\n",
      "julenaino\n",
      "anovan\n",
      "idriamy\n",
      "te\n",
      "stan\n",
      "niko\n",
      "cesuke\n",
      "sii\n",
      "dayvin\n",
      "emma\n",
      "angelier\n",
      "romerantk\n",
      "ana\n",
      "keralynn\n",
      "mael\n",
      "jolaya\n",
      "jakaizin\n",
      "jacella\n"
     ]
    }
   ],
   "source": [
    "# generate a bunch of names\n",
    "for i in range(20):\n",
    "    print(generate_word_nn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
