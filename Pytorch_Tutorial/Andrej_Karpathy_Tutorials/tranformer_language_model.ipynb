{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Language Model (based on Adrej Karpathy's nanoGPT tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1234)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the txt entire file into a single string\n",
    "with open('tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total length of dataset in characters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character vocabulary:  ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# get vocabulary of characters\n",
    "vocab = sorted(set(list(text)))\n",
    "print(\"character vocabulary: \", vocab)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "ctoi = {vocab[i]:i for i in range(vocab_size)}\n",
    "itoc = {i:vocab[i] for i in range(vocab_size)}\n",
    "encode = lambda s: [ctoi[c] for c in s]  # converts a string to integer token sequence\n",
    "decode = lambda s: [itoc[ix] for ix in s]  # converts an integer token sequence to string of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2]\n",
      "['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!']\n"
     ]
    }
   ],
   "source": [
    "print(encode('Hello world!'))\n",
    "print(decode(encode('Hello world!')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# tokenize the dataset into integer sequence, convert to torch tensor of type int64\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long) \n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-validation splits (90-10)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the data into chuncks of size block_size. For each chunk, we create (input,target) pairs for next character prediction, where the input is a context window containing all characters preceding the target character. Note that the context sizes range from 1 up to block size, i.e. there will be block_size number of (input,target) pairs per chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: tensor([18]) --> target: 47\n",
      "Context: tensor([18, 47]) --> target: 56\n",
      "Context: tensor([18, 47, 56]) --> target: 57\n",
      "Context: tensor([18, 47, 56, 57]) --> target: 58\n",
      "Context: tensor([18, 47, 56, 57, 58]) --> target: 1\n",
      "Context: tensor([18, 47, 56, 57, 58,  1]) --> target: 15\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15]) --> target: 47\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47]) --> target: 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "# example showing the first chunk and all possible (input,target) pairs we can get from it\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Context: {context} --> target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create a batch generator which creates a batch of randomly selected blocks/chunks from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch: \n",
      "torch.Size([4, 8])\n",
      "tensor([[17, 17, 26,  1, 17, 24, 21, 38],\n",
      "        [14, 17, 24, 24, 13, 10,  0, 28],\n",
      "        [63,  1, 47, 52,  1, 56, 43, 55],\n",
      "        [56, 59, 57, 58,  1, 59, 54, 53]], device='cuda:0')\n",
      "target batch: \n",
      "torch.Size([4, 8])\n",
      "tensor([[17, 26,  1, 17, 24, 21, 38, 13],\n",
      "        [17, 24, 24, 13, 10,  0, 28, 50],\n",
      "        [ 1, 47, 52,  1, 56, 43, 55, 59],\n",
      "        [59, 57, 58,  1, 59, 54, 53, 52]], device='cuda:0')\n",
      "\n",
      "A batch of 4 blocks:\n",
      "\n",
      "Block 0:\n",
      "Context: [17] --> target: 17\n",
      "Context: [17, 17] --> target: 26\n",
      "Context: [17, 17, 26] --> target: 1\n",
      "Context: [17, 17, 26, 1] --> target: 17\n",
      "Context: [17, 17, 26, 1, 17] --> target: 24\n",
      "Context: [17, 17, 26, 1, 17, 24] --> target: 21\n",
      "Context: [17, 17, 26, 1, 17, 24, 21] --> target: 38\n",
      "Context: [17, 17, 26, 1, 17, 24, 21, 38] --> target: 13\n",
      "\n",
      "\n",
      "Block 1:\n",
      "Context: [14] --> target: 17\n",
      "Context: [14, 17] --> target: 24\n",
      "Context: [14, 17, 24] --> target: 24\n",
      "Context: [14, 17, 24, 24] --> target: 13\n",
      "Context: [14, 17, 24, 24, 13] --> target: 10\n",
      "Context: [14, 17, 24, 24, 13, 10] --> target: 0\n",
      "Context: [14, 17, 24, 24, 13, 10, 0] --> target: 28\n",
      "Context: [14, 17, 24, 24, 13, 10, 0, 28] --> target: 50\n",
      "\n",
      "\n",
      "Block 2:\n",
      "Context: [63] --> target: 1\n",
      "Context: [63, 1] --> target: 47\n",
      "Context: [63, 1, 47] --> target: 52\n",
      "Context: [63, 1, 47, 52] --> target: 1\n",
      "Context: [63, 1, 47, 52, 1] --> target: 56\n",
      "Context: [63, 1, 47, 52, 1, 56] --> target: 43\n",
      "Context: [63, 1, 47, 52, 1, 56, 43] --> target: 55\n",
      "Context: [63, 1, 47, 52, 1, 56, 43, 55] --> target: 59\n",
      "\n",
      "\n",
      "Block 3:\n",
      "Context: [56] --> target: 59\n",
      "Context: [56, 59] --> target: 57\n",
      "Context: [56, 59, 57] --> target: 58\n",
      "Context: [56, 59, 57, 58] --> target: 1\n",
      "Context: [56, 59, 57, 58, 1] --> target: 59\n",
      "Context: [56, 59, 57, 58, 1, 59] --> target: 54\n",
      "Context: [56, 59, 57, 58, 1, 59, 54] --> target: 53\n",
      "Context: [56, 59, 57, 58, 1, 59, 54, 53] --> target: 52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1223)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "# data loader (generates a bvatch of randomly selected blocks)\n",
    "def get_batch(split='train'):\n",
    "    data = train_data if split=='train' else val_data\n",
    "\n",
    "    # sample positions from which to grab blocks\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])      \n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # move tensors to gpu \n",
    "    return x,y \n",
    "\n",
    "xbatch, ybatch = get_batch('train')\n",
    "print(\"input batch: \")\n",
    "print(xbatch.shape)\n",
    "print(xbatch)\n",
    "print(\"target batch: \")\n",
    "print(ybatch.shape)     \n",
    "print(ybatch)     \n",
    "print(\"\")\n",
    "\n",
    "# context target pairs\n",
    "print(f\"A batch of {batch_size} blocks:\")\n",
    "for b in range(batch_size): # batch dimension\n",
    "    print(f\"\\nBlock {b}:\")\n",
    "    for t in range(block_size):  # time dimension\n",
    "        context = xbatch[b,:t+1]\n",
    "        target = ybatch[b,t]\n",
    "        print(f\"Context: {context.tolist()} --> target: {target}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's create a pytorch-ified Bi-gram language model (will serve as a baseline for comparing the transformer model later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "batch_size = 64\n",
    "block_size = 8\n",
    "max_iters = 6000\n",
    "learning_rate = 1e-2\n",
    "eval_interval = 300\n",
    "eval_iters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # lookup table for finding logits for the next token (i.e. log of counts for all possible next token given input token)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # shape: (C,C)\n",
    "\n",
    "\n",
    "    # forward pass takes in a batch of input token sequences of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, targets=None):\n",
    "        # get logits for every input token\n",
    "        logits = self.token_embedding_table(idx) # shape: (B,T,C)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,C = logits.shape\n",
    "            # reshape the logits and targets such that batch of input sequences are flattened into a single big input sequence\n",
    "            # i.e. (B,T) --> (B*T)\n",
    "            logits = logits.view(B*T,C) # reshaped to (B*T,C)\n",
    "            targets = targets.view(B*T) # reshaped to (B*T)\n",
    "            # compute cross entropy loss (i.e. average negative log likelihood)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    # generates new sequences continuing from a given batch of context tokens\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # batch of contexts, idx has shape (B,T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, _ = self(idx) # shape: (B,T,C)\n",
    "            # for each context sequence (in the batch), compute the probability of the next token using the logits of the last token in the context sequence\n",
    "            logits = logits[:,-1,:] # shape: (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            # sample from the probability distribution to get next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape: (B,1)\n",
    "            # append to the current context\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # shape: (B,T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.5195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Generated sequence:\n",
      " \n",
      "CPryCqOOqL 'xTaAvrDiqnGa$PxT\n",
      "?k3Jy$vjOpus.ca$KsY?u?s$-qwc: !!rzuniPGJ'qvs3VMkxHUWY qeqmefwjYG,LLZ\n",
      "xM\n"
     ]
    }
   ],
   "source": [
    "# create a bigram language model and test it on the example batch\n",
    "model = BigramLanguageModel(vocab_size=vocab_size)\n",
    "# move model to device\n",
    "m = model.to(device)\n",
    "logits, loss = m(xbatch, ybatch)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# generate a single sequences using the model with start token 0\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_seq = m.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "# Decode integer tokens into characters\n",
    "generated_seq = decode(generated_seq)\n",
    "print(\"\\nGenerated sequence:\\n\",\"\".join(generated_seq))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated sequence looks like gibberish, because model is untrained. We now train the model using a graident based optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating training and validation losses averaged over lots of batches\n",
    "@torch.no_grad() # disable gradient tracking\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() # swicth to inference mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split) \n",
    "            _, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() \n",
    "    model.train() # switch back to training mode\n",
    "    return out       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training loss: 4.615415573120117, validation loss: 4.633676052093506\n",
      "epoch: 300, training loss: 2.7299535274505615, validation loss: 2.7564690113067627\n",
      "epoch: 600, training loss: 2.519435167312622, validation loss: 2.5423777103424072\n",
      "epoch: 900, training loss: 2.488534927368164, validation loss: 2.5016705989837646\n",
      "epoch: 1200, training loss: 2.4763269424438477, validation loss: 2.4994447231292725\n",
      "epoch: 1500, training loss: 2.4710662364959717, validation loss: 2.4984192848205566\n",
      "epoch: 1800, training loss: 2.466684103012085, validation loss: 2.4925339221954346\n",
      "epoch: 2100, training loss: 2.467583656311035, validation loss: 2.4871132373809814\n",
      "epoch: 2400, training loss: 2.4666929244995117, validation loss: 2.4932312965393066\n",
      "epoch: 2700, training loss: 2.459779739379883, validation loss: 2.485511064529419\n",
      "epoch: 3000, training loss: 2.454580783843994, validation loss: 2.482884168624878\n",
      "epoch: 3300, training loss: 2.4595024585723877, validation loss: 2.4840993881225586\n",
      "epoch: 3600, training loss: 2.458650588989258, validation loss: 2.491469144821167\n",
      "epoch: 3900, training loss: 2.460474967956543, validation loss: 2.4808499813079834\n",
      "epoch: 4200, training loss: 2.468102216720581, validation loss: 2.486747980117798\n",
      "epoch: 4500, training loss: 2.457486867904663, validation loss: 2.485264778137207\n",
      "epoch: 4800, training loss: 2.458542823791504, validation loss: 2.4905202388763428\n",
      "epoch: 5100, training loss: 2.4552981853485107, validation loss: 2.4863624572753906\n",
      "epoch: 5400, training loss: 2.4497551918029785, validation loss: 2.488884210586548\n",
      "epoch: 5700, training loss: 2.4539077281951904, validation loss: 2.4895074367523193\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_iters):\n",
    "    # sample a batch of trainin data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    _, loss = m(xb, yb)\n",
    "    # reset parameter gradients\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"epoch: {epoch}, training loss: {losses['train'].item()}, validation loss: {losses['val'].item()}\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try generating some text using the trained bigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sequence:\n",
      " \n",
      "TI owind,\n",
      "\n",
      "Y:\n",
      "A:\n",
      "F awind fllaverat heroshilds CEdd n!\n",
      "ISarext me s merd; w klishe t in\n",
      "Thes\n",
      "TE k s t thaifor than OLo, s th t ng, ffeey\n",
      "AS:\n",
      "Foeers ordld ann d itou makenche thir in. pr.\n",
      "Hot h po cceplcy aue as YCE t! the\n",
      "Th--\n",
      "\n",
      "\n",
      "Thinongamarearr thes ci.\n",
      "fekesmy tot eritond e;\n",
      "Whyoke aknthin; y w,-h; \n"
     ]
    }
   ],
   "source": [
    "# generate a single sequences using the model with start token 0\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_seq = m.generate(idx, max_new_tokens=300)[0].tolist()\n",
    "# Decode integer tokens into characters\n",
    "generated_seq = decode(generated_seq)\n",
    "print(\"\\nGenerated sequence:\\n\",\"\".join(generated_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better! It has similar syntactic structure as the training text and even has some correct words. The quality is still very bad because the context window is too small, only the previous character is used to predict the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
