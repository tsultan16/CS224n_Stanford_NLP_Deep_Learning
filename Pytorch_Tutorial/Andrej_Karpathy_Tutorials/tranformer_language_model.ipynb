{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Language Model (based on Adrej Karpathy's nanoGPT tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the txt entire file into a single string\n",
    "with open('tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total length of dataset in characters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character vocabulary:  ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# get vocabulary of characters\n",
    "vocab = sorted(set(list(text)))\n",
    "print(\"character vocabulary: \", vocab)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "ctoi = {vocab[i]:i for i in range(vocab_size)}\n",
    "itoc = {i:vocab[i] for i in range(vocab_size)}\n",
    "encode = lambda s: [ctoi[c] for c in s]  # converts a string to integer token sequence\n",
    "decode = lambda s: [itoc[ix] for ix in s]  # converts an integer token sequence to string of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2]\n",
      "['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!']\n"
     ]
    }
   ],
   "source": [
    "print(encode('Hello world!'))\n",
    "print(decode(encode('Hello world!')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# tokenize the dataset into integer sequence, convert to torch tensor of type int64\n",
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long) \n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-validation splits (90-10)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the data into chuncks of size block_size. For each chunk, we create (input,target) pairs for next character prediction, where the input is a context window containing all characters preceding the target character. Note that the context sizes range from 1 up to block size, i.e. there will be block_size number of (input,target) pairs per chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: tensor([18]) --> target: 47\n",
      "Context: tensor([18, 47]) --> target: 56\n",
      "Context: tensor([18, 47, 56]) --> target: 57\n",
      "Context: tensor([18, 47, 56, 57]) --> target: 58\n",
      "Context: tensor([18, 47, 56, 57, 58]) --> target: 1\n",
      "Context: tensor([18, 47, 56, 57, 58,  1]) --> target: 15\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15]) --> target: 47\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47]) --> target: 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "# example showing the first chunk and all possible (input,target) pairs we can get from it\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Context: {context} --> target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create a batch generator which creates a batch of randomly selected blocks/chunks from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch: \n",
      "torch.Size([4, 8])\n",
      "tensor([[17, 17, 26,  1, 17, 24, 21, 38],\n",
      "        [14, 17, 24, 24, 13, 10,  0, 28],\n",
      "        [63,  1, 47, 52,  1, 56, 43, 55],\n",
      "        [56, 59, 57, 58,  1, 59, 54, 53]])\n",
      "target batch: \n",
      "torch.Size([4, 8])\n",
      "tensor([[17, 26,  1, 17, 24, 21, 38, 13],\n",
      "        [17, 24, 24, 13, 10,  0, 28, 50],\n",
      "        [ 1, 47, 52,  1, 56, 43, 55, 59],\n",
      "        [59, 57, 58,  1, 59, 54, 53, 52]])\n",
      "\n",
      "A batch of 4 blocks:\n",
      "\n",
      "Block 0:\n",
      "Context: [17] --> target: 17\n",
      "Context: [17, 17] --> target: 26\n",
      "Context: [17, 17, 26] --> target: 1\n",
      "Context: [17, 17, 26, 1] --> target: 17\n",
      "Context: [17, 17, 26, 1, 17] --> target: 24\n",
      "Context: [17, 17, 26, 1, 17, 24] --> target: 21\n",
      "Context: [17, 17, 26, 1, 17, 24, 21] --> target: 38\n",
      "Context: [17, 17, 26, 1, 17, 24, 21, 38] --> target: 13\n",
      "\n",
      "\n",
      "Block 1:\n",
      "Context: [14] --> target: 17\n",
      "Context: [14, 17] --> target: 24\n",
      "Context: [14, 17, 24] --> target: 24\n",
      "Context: [14, 17, 24, 24] --> target: 13\n",
      "Context: [14, 17, 24, 24, 13] --> target: 10\n",
      "Context: [14, 17, 24, 24, 13, 10] --> target: 0\n",
      "Context: [14, 17, 24, 24, 13, 10, 0] --> target: 28\n",
      "Context: [14, 17, 24, 24, 13, 10, 0, 28] --> target: 50\n",
      "\n",
      "\n",
      "Block 2:\n",
      "Context: [63] --> target: 1\n",
      "Context: [63, 1] --> target: 47\n",
      "Context: [63, 1, 47] --> target: 52\n",
      "Context: [63, 1, 47, 52] --> target: 1\n",
      "Context: [63, 1, 47, 52, 1] --> target: 56\n",
      "Context: [63, 1, 47, 52, 1, 56] --> target: 43\n",
      "Context: [63, 1, 47, 52, 1, 56, 43] --> target: 55\n",
      "Context: [63, 1, 47, 52, 1, 56, 43, 55] --> target: 59\n",
      "\n",
      "\n",
      "Block 3:\n",
      "Context: [56] --> target: 59\n",
      "Context: [56, 59] --> target: 57\n",
      "Context: [56, 59, 57] --> target: 58\n",
      "Context: [56, 59, 57, 58] --> target: 1\n",
      "Context: [56, 59, 57, 58, 1] --> target: 59\n",
      "Context: [56, 59, 57, 58, 1, 59] --> target: 54\n",
      "Context: [56, 59, 57, 58, 1, 59, 54] --> target: 53\n",
      "Context: [56, 59, 57, 58, 1, 59, 54, 53] --> target: 52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1223)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split='train'):\n",
    "    data = train_data if split=='train' else val_data\n",
    "\n",
    "    # sample positions from which to grab blocks\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])      \n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x,y \n",
    "\n",
    "xbatch, ybatch = get_batch('train')\n",
    "print(\"input batch: \")\n",
    "print(xbatch.shape)\n",
    "print(xbatch)\n",
    "print(\"target batch: \")\n",
    "print(ybatch.shape)     \n",
    "print(ybatch)     \n",
    "print(\"\")\n",
    "\n",
    "# context target pairs\n",
    "print(f\"A batch of {batch_size} blocks:\")\n",
    "for b in range(batch_size): # batch dimension\n",
    "    print(f\"\\nBlock {b}:\")\n",
    "    for t in range(block_size):  # time dimension\n",
    "        context = xbatch[b,:t+1]\n",
    "        target = ybatch[b,t]\n",
    "        print(f\"Context: {context.tolist()} --> target: {target}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's create a pytorch-ified Bi-gram language model (will serve as a baseline for comparing the transformer model later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # lookup table for finding logits for the next token (i.e. log of counts for all possible next token given input token)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # shape: (C,C)\n",
    "\n",
    "\n",
    "    # forward pass takes in a batch of input token sequences of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, targets=None):\n",
    "        # get logits for every input token\n",
    "        logits = self.token_embedding_table(idx) # shape: (B,T,C)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,C = logits.shape\n",
    "            # reshape the logits and targets such that batch of input sequences are flattened into a single big input sequence\n",
    "            # i.e. (B,T) --> (B*T)\n",
    "            logits = logits.view(B*T,C) # reshaped to (B*T,C)\n",
    "            targets = targets.view(B*T) # reshaped to (B*T)\n",
    "            # compute cross entropy loss (i.e. average negative log likelihood)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    # generates new sequences continuing from a given batch of context tokens\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # batch of contexts, idx has shape (B,T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, _ = self(idx) # shape: (B,T,C)\n",
    "            # for each context sequence (in the batch), compute the probability of the next token using the logits of the last token in the context sequence\n",
    "            logits = logits[:,-1,:] # shape: (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            # sample from the probability distribution to get next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape: (B,1)\n",
    "            # append to the current context\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # shape: (B,T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.9995, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Generated sequence:\n",
      " \n",
      "W?MwDJJwiFUs&vgOaq$KLpDBQRCMw\n",
      "UVVoH3GYFUMIvMMw!rEOAYFbvdvSYw3?E!s uQFhq''Vutws$F&\n",
      "jk,hH,eTCDH\n",
      "A\n",
      "yY.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a bigram language model and test it on the example batch\n",
    "m = BigramLanguageModel(vocab_size=vocab_size)\n",
    "logits, loss = m(xbatch, ybatch)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# generate a single sequences using the model with start token 0\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "generated_seq = m.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "# Decode integer tokens into characters\n",
    "generated_seq = decode(generated_seq)\n",
    "print(\"\\nGenerated sequence:\\n\",\"\".join(generated_seq))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated sequence looks like gibberish, because model is untrained. We now train the model using a graident based optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training loss: 2.653836488723755\n",
      "epoch: 10, training loss: 2.7165653705596924\n",
      "epoch: 20, training loss: 2.7700629234313965\n",
      "epoch: 30, training loss: 2.7690882682800293\n",
      "epoch: 40, training loss: 2.727691173553467\n",
      "epoch: 50, training loss: 2.736506700515747\n",
      "epoch: 60, training loss: 2.7170002460479736\n",
      "epoch: 70, training loss: 2.679131031036377\n",
      "epoch: 80, training loss: 2.7840893268585205\n",
      "epoch: 90, training loss: 2.707040309906006\n",
      "epoch: 100, training loss: 2.682605028152466\n",
      "epoch: 110, training loss: 2.7026965618133545\n",
      "epoch: 120, training loss: 2.726210117340088\n",
      "epoch: 130, training loss: 2.7604587078094482\n",
      "epoch: 140, training loss: 2.6572349071502686\n",
      "epoch: 150, training loss: 2.6831789016723633\n",
      "epoch: 160, training loss: 2.6536099910736084\n",
      "epoch: 170, training loss: 2.701310634613037\n",
      "epoch: 180, training loss: 2.738854169845581\n",
      "epoch: 190, training loss: 2.7191481590270996\n",
      "epoch: 200, training loss: 2.6766655445098877\n",
      "epoch: 210, training loss: 2.7442188262939453\n",
      "epoch: 220, training loss: 2.717247247695923\n",
      "epoch: 230, training loss: 2.690523147583008\n",
      "epoch: 240, training loss: 2.6200673580169678\n",
      "epoch: 250, training loss: 2.7070963382720947\n",
      "epoch: 260, training loss: 2.688159704208374\n",
      "epoch: 270, training loss: 2.6423325538635254\n",
      "epoch: 280, training loss: 2.7391624450683594\n",
      "epoch: 290, training loss: 2.731096029281616\n",
      "epoch: 300, training loss: 2.6762216091156006\n",
      "epoch: 310, training loss: 2.5634350776672363\n",
      "epoch: 320, training loss: 2.712573528289795\n",
      "epoch: 330, training loss: 2.6706349849700928\n",
      "epoch: 340, training loss: 2.680553913116455\n",
      "epoch: 350, training loss: 2.686980962753296\n",
      "epoch: 360, training loss: 2.6764042377471924\n",
      "epoch: 370, training loss: 2.5943355560302734\n",
      "epoch: 380, training loss: 2.552905797958374\n",
      "epoch: 390, training loss: 2.670588493347168\n",
      "epoch: 400, training loss: 2.622114658355713\n",
      "epoch: 410, training loss: 2.661405324935913\n",
      "epoch: 420, training loss: 2.6460964679718018\n",
      "epoch: 430, training loss: 2.6901023387908936\n",
      "epoch: 440, training loss: 2.6930441856384277\n",
      "epoch: 450, training loss: 2.6198058128356934\n",
      "epoch: 460, training loss: 2.6471621990203857\n",
      "epoch: 470, training loss: 2.6377971172332764\n",
      "epoch: 480, training loss: 2.639878034591675\n",
      "epoch: 490, training loss: 2.610347032546997\n",
      "epoch: 500, training loss: 2.7214887142181396\n",
      "epoch: 510, training loss: 2.6079134941101074\n",
      "epoch: 520, training loss: 2.6754791736602783\n",
      "epoch: 530, training loss: 2.647193670272827\n",
      "epoch: 540, training loss: 2.643031358718872\n",
      "epoch: 550, training loss: 2.6279032230377197\n",
      "epoch: 560, training loss: 2.6609442234039307\n",
      "epoch: 570, training loss: 2.623382806777954\n",
      "epoch: 580, training loss: 2.672070264816284\n",
      "epoch: 590, training loss: 2.594177007675171\n",
      "epoch: 600, training loss: 2.6052935123443604\n",
      "epoch: 610, training loss: 2.675428628921509\n",
      "epoch: 620, training loss: 2.57611083984375\n",
      "epoch: 630, training loss: 2.6023168563842773\n",
      "epoch: 640, training loss: 2.6643245220184326\n",
      "epoch: 650, training loss: 2.592106819152832\n",
      "epoch: 660, training loss: 2.6365370750427246\n",
      "epoch: 670, training loss: 2.6561806201934814\n",
      "epoch: 680, training loss: 2.726987838745117\n",
      "epoch: 690, training loss: 2.6162030696868896\n",
      "epoch: 700, training loss: 2.6607511043548584\n",
      "epoch: 710, training loss: 2.5571129322052\n",
      "epoch: 720, training loss: 2.5931661128997803\n",
      "epoch: 730, training loss: 2.61689829826355\n",
      "epoch: 740, training loss: 2.6841490268707275\n",
      "epoch: 750, training loss: 2.6188974380493164\n",
      "epoch: 760, training loss: 2.6494059562683105\n",
      "epoch: 770, training loss: 2.642509937286377\n",
      "epoch: 780, training loss: 2.590742826461792\n",
      "epoch: 790, training loss: 2.6539499759674072\n",
      "epoch: 800, training loss: 2.4961698055267334\n",
      "epoch: 810, training loss: 2.595066785812378\n",
      "epoch: 820, training loss: 2.5842928886413574\n",
      "epoch: 830, training loss: 2.572376012802124\n",
      "epoch: 840, training loss: 2.6159794330596924\n",
      "epoch: 850, training loss: 2.5809316635131836\n",
      "epoch: 860, training loss: 2.5725913047790527\n",
      "epoch: 870, training loss: 2.594345808029175\n",
      "epoch: 880, training loss: 2.659101963043213\n",
      "epoch: 890, training loss: 2.528397798538208\n",
      "epoch: 900, training loss: 2.4643714427948\n",
      "epoch: 910, training loss: 2.5828564167022705\n",
      "epoch: 920, training loss: 2.562847852706909\n",
      "epoch: 930, training loss: 2.590684175491333\n",
      "epoch: 940, training loss: 2.6422791481018066\n",
      "epoch: 950, training loss: 2.5904738903045654\n",
      "epoch: 960, training loss: 2.5668559074401855\n",
      "epoch: 970, training loss: 2.651841640472412\n",
      "epoch: 980, training loss: 2.536985397338867\n",
      "epoch: 990, training loss: 2.657824993133545\n",
      "epoch: 1000, training loss: 2.6533091068267822\n",
      "epoch: 1010, training loss: 2.6085429191589355\n",
      "epoch: 1020, training loss: 2.5815749168395996\n",
      "epoch: 1030, training loss: 2.537811756134033\n",
      "epoch: 1040, training loss: 2.568769931793213\n",
      "epoch: 1050, training loss: 2.623201608657837\n",
      "epoch: 1060, training loss: 2.658123731613159\n",
      "epoch: 1070, training loss: 2.6454403400421143\n",
      "epoch: 1080, training loss: 2.6129376888275146\n",
      "epoch: 1090, training loss: 2.613507032394409\n",
      "epoch: 1100, training loss: 2.5793824195861816\n",
      "epoch: 1110, training loss: 2.5630905628204346\n",
      "epoch: 1120, training loss: 2.5982754230499268\n",
      "epoch: 1130, training loss: 2.5126068592071533\n",
      "epoch: 1140, training loss: 2.537182569503784\n",
      "epoch: 1150, training loss: 2.6668636798858643\n",
      "epoch: 1160, training loss: 2.6421728134155273\n",
      "epoch: 1170, training loss: 2.544321060180664\n",
      "epoch: 1180, training loss: 2.553115129470825\n",
      "epoch: 1190, training loss: 2.6156423091888428\n",
      "epoch: 1200, training loss: 2.581800699234009\n",
      "epoch: 1210, training loss: 2.590184450149536\n",
      "epoch: 1220, training loss: 2.562952756881714\n",
      "epoch: 1230, training loss: 2.581977605819702\n",
      "epoch: 1240, training loss: 2.534869909286499\n",
      "epoch: 1250, training loss: 2.6014246940612793\n",
      "epoch: 1260, training loss: 2.549818992614746\n",
      "epoch: 1270, training loss: 2.5606637001037598\n",
      "epoch: 1280, training loss: 2.493027448654175\n",
      "epoch: 1290, training loss: 2.5795469284057617\n",
      "epoch: 1300, training loss: 2.5234358310699463\n",
      "epoch: 1310, training loss: 2.447666883468628\n",
      "epoch: 1320, training loss: 2.5823867321014404\n",
      "epoch: 1330, training loss: 2.5726470947265625\n",
      "epoch: 1340, training loss: 2.536529779434204\n",
      "epoch: 1350, training loss: 2.552788734436035\n",
      "epoch: 1360, training loss: 2.6157965660095215\n",
      "epoch: 1370, training loss: 2.6066436767578125\n",
      "epoch: 1380, training loss: 2.5109856128692627\n",
      "epoch: 1390, training loss: 2.560372829437256\n",
      "epoch: 1400, training loss: 2.4895620346069336\n",
      "epoch: 1410, training loss: 2.5699715614318848\n",
      "epoch: 1420, training loss: 2.6441280841827393\n",
      "epoch: 1430, training loss: 2.538872003555298\n",
      "epoch: 1440, training loss: 2.509896755218506\n",
      "epoch: 1450, training loss: 2.6632466316223145\n",
      "epoch: 1460, training loss: 2.500046730041504\n",
      "epoch: 1470, training loss: 2.490709066390991\n",
      "epoch: 1480, training loss: 2.550011157989502\n",
      "epoch: 1490, training loss: 2.634061574935913\n",
      "epoch: 1500, training loss: 2.5258426666259766\n",
      "epoch: 1510, training loss: 2.5743093490600586\n",
      "epoch: 1520, training loss: 2.5762619972229004\n",
      "epoch: 1530, training loss: 2.6512084007263184\n",
      "epoch: 1540, training loss: 2.6818153858184814\n",
      "epoch: 1550, training loss: 2.540696382522583\n",
      "epoch: 1560, training loss: 2.5188398361206055\n",
      "epoch: 1570, training loss: 2.574982166290283\n",
      "epoch: 1580, training loss: 2.5694262981414795\n",
      "epoch: 1590, training loss: 2.596390724182129\n",
      "epoch: 1600, training loss: 2.4511935710906982\n",
      "epoch: 1610, training loss: 2.5118186473846436\n",
      "epoch: 1620, training loss: 2.56272292137146\n",
      "epoch: 1630, training loss: 2.4723362922668457\n",
      "epoch: 1640, training loss: 2.582437515258789\n",
      "epoch: 1650, training loss: 2.576932668685913\n",
      "epoch: 1660, training loss: 2.499232530593872\n",
      "epoch: 1670, training loss: 2.5564661026000977\n",
      "epoch: 1680, training loss: 2.559014320373535\n",
      "epoch: 1690, training loss: 2.5060791969299316\n",
      "epoch: 1700, training loss: 2.459644079208374\n",
      "epoch: 1710, training loss: 2.535825490951538\n",
      "epoch: 1720, training loss: 2.548659086227417\n",
      "epoch: 1730, training loss: 2.6025805473327637\n",
      "epoch: 1740, training loss: 2.5779261589050293\n",
      "epoch: 1750, training loss: 2.502864122390747\n",
      "epoch: 1760, training loss: 2.5232138633728027\n",
      "epoch: 1770, training loss: 2.5743327140808105\n",
      "epoch: 1780, training loss: 2.5312347412109375\n",
      "epoch: 1790, training loss: 2.5697574615478516\n",
      "epoch: 1800, training loss: 2.549426555633545\n",
      "epoch: 1810, training loss: 2.516669988632202\n",
      "epoch: 1820, training loss: 2.5749974250793457\n",
      "epoch: 1830, training loss: 2.5713729858398438\n",
      "epoch: 1840, training loss: 2.5034005641937256\n",
      "epoch: 1850, training loss: 2.508072853088379\n",
      "epoch: 1860, training loss: 2.4944944381713867\n",
      "epoch: 1870, training loss: 2.5426559448242188\n",
      "epoch: 1880, training loss: 2.583397388458252\n",
      "epoch: 1890, training loss: 2.52067494392395\n",
      "epoch: 1900, training loss: 2.6185505390167236\n",
      "epoch: 1910, training loss: 2.5383667945861816\n",
      "epoch: 1920, training loss: 2.536393165588379\n",
      "epoch: 1930, training loss: 2.5110437870025635\n",
      "epoch: 1940, training loss: 2.594379425048828\n",
      "epoch: 1950, training loss: 2.563962936401367\n",
      "epoch: 1960, training loss: 2.5754287242889404\n",
      "epoch: 1970, training loss: 2.445777654647827\n",
      "epoch: 1980, training loss: 2.539808988571167\n",
      "epoch: 1990, training loss: 2.541058301925659\n",
      "epoch: 2000, training loss: 2.503450632095337\n",
      "epoch: 2010, training loss: 2.5290110111236572\n",
      "epoch: 2020, training loss: 2.512047529220581\n",
      "epoch: 2030, training loss: 2.521678924560547\n",
      "epoch: 2040, training loss: 2.509080171585083\n",
      "epoch: 2050, training loss: 2.494004011154175\n",
      "epoch: 2060, training loss: 2.4677789211273193\n",
      "epoch: 2070, training loss: 2.521693468093872\n",
      "epoch: 2080, training loss: 2.572537899017334\n",
      "epoch: 2090, training loss: 2.5463919639587402\n",
      "epoch: 2100, training loss: 2.5798799991607666\n",
      "epoch: 2110, training loss: 2.56136155128479\n",
      "epoch: 2120, training loss: 2.5347495079040527\n",
      "epoch: 2130, training loss: 2.5839717388153076\n",
      "epoch: 2140, training loss: 2.5564935207366943\n",
      "epoch: 2150, training loss: 2.5855038166046143\n",
      "epoch: 2160, training loss: 2.579305648803711\n",
      "epoch: 2170, training loss: 2.591235876083374\n",
      "epoch: 2180, training loss: 2.522279739379883\n",
      "epoch: 2190, training loss: 2.590806722640991\n",
      "epoch: 2200, training loss: 2.5532777309417725\n",
      "epoch: 2210, training loss: 2.5667691230773926\n",
      "epoch: 2220, training loss: 2.5806028842926025\n",
      "epoch: 2230, training loss: 2.531424045562744\n",
      "epoch: 2240, training loss: 2.4816603660583496\n",
      "epoch: 2250, training loss: 2.4369959831237793\n",
      "epoch: 2260, training loss: 2.5190987586975098\n",
      "epoch: 2270, training loss: 2.494523286819458\n",
      "epoch: 2280, training loss: 2.5379714965820312\n",
      "epoch: 2290, training loss: 2.491507053375244\n",
      "epoch: 2300, training loss: 2.474229574203491\n",
      "epoch: 2310, training loss: 2.5927178859710693\n",
      "epoch: 2320, training loss: 2.5084497928619385\n",
      "epoch: 2330, training loss: 2.521484375\n",
      "epoch: 2340, training loss: 2.5188400745391846\n",
      "epoch: 2350, training loss: 2.501610279083252\n",
      "epoch: 2360, training loss: 2.487213611602783\n",
      "epoch: 2370, training loss: 2.501163959503174\n",
      "epoch: 2380, training loss: 2.567354440689087\n",
      "epoch: 2390, training loss: 2.5482912063598633\n",
      "epoch: 2400, training loss: 2.473827838897705\n",
      "epoch: 2410, training loss: 2.4724438190460205\n",
      "epoch: 2420, training loss: 2.4041497707366943\n",
      "epoch: 2430, training loss: 2.577758550643921\n",
      "epoch: 2440, training loss: 2.5431268215179443\n",
      "epoch: 2450, training loss: 2.623447895050049\n",
      "epoch: 2460, training loss: 2.511533737182617\n",
      "epoch: 2470, training loss: 2.49678111076355\n",
      "epoch: 2480, training loss: 2.378229856491089\n",
      "epoch: 2490, training loss: 2.5504801273345947\n",
      "epoch: 2500, training loss: 2.462345838546753\n",
      "epoch: 2510, training loss: 2.5032057762145996\n",
      "epoch: 2520, training loss: 2.5403642654418945\n",
      "epoch: 2530, training loss: 2.498957395553589\n",
      "epoch: 2540, training loss: 2.541558027267456\n",
      "epoch: 2550, training loss: 2.4797751903533936\n",
      "epoch: 2560, training loss: 2.497636318206787\n",
      "epoch: 2570, training loss: 2.411257028579712\n",
      "epoch: 2580, training loss: 2.468991994857788\n",
      "epoch: 2590, training loss: 2.494870185852051\n",
      "epoch: 2600, training loss: 2.535665273666382\n",
      "epoch: 2610, training loss: 2.4870445728302\n",
      "epoch: 2620, training loss: 2.5053725242614746\n",
      "epoch: 2630, training loss: 2.5153753757476807\n",
      "epoch: 2640, training loss: 2.520059823989868\n",
      "epoch: 2650, training loss: 2.5671050548553467\n",
      "epoch: 2660, training loss: 2.6393837928771973\n",
      "epoch: 2670, training loss: 2.564093589782715\n",
      "epoch: 2680, training loss: 2.4506995677948\n",
      "epoch: 2690, training loss: 2.534541606903076\n",
      "epoch: 2700, training loss: 2.522660732269287\n",
      "epoch: 2710, training loss: 2.572993040084839\n",
      "epoch: 2720, training loss: 2.4978818893432617\n",
      "epoch: 2730, training loss: 2.500631332397461\n",
      "epoch: 2740, training loss: 2.4566490650177\n",
      "epoch: 2750, training loss: 2.4454662799835205\n",
      "epoch: 2760, training loss: 2.459547758102417\n",
      "epoch: 2770, training loss: 2.503964900970459\n",
      "epoch: 2780, training loss: 2.468090295791626\n",
      "epoch: 2790, training loss: 2.486119031906128\n",
      "epoch: 2800, training loss: 2.5330190658569336\n",
      "epoch: 2810, training loss: 2.5898427963256836\n",
      "epoch: 2820, training loss: 2.4626119136810303\n",
      "epoch: 2830, training loss: 2.495266914367676\n",
      "epoch: 2840, training loss: 2.4714629650115967\n",
      "epoch: 2850, training loss: 2.4705677032470703\n",
      "epoch: 2860, training loss: 2.4541172981262207\n",
      "epoch: 2870, training loss: 2.5799973011016846\n",
      "epoch: 2880, training loss: 2.4381790161132812\n",
      "epoch: 2890, training loss: 2.4590017795562744\n",
      "epoch: 2900, training loss: 2.5580484867095947\n",
      "epoch: 2910, training loss: 2.572756052017212\n",
      "epoch: 2920, training loss: 2.5085198879241943\n",
      "epoch: 2930, training loss: 2.436567783355713\n",
      "epoch: 2940, training loss: 2.560821533203125\n",
      "epoch: 2950, training loss: 2.4557416439056396\n",
      "epoch: 2960, training loss: 2.4455738067626953\n",
      "epoch: 2970, training loss: 2.4139935970306396\n",
      "epoch: 2980, training loss: 2.5108847618103027\n",
      "epoch: 2990, training loss: 2.427700996398926\n",
      "epoch: 3000, training loss: 2.487055778503418\n",
      "epoch: 3010, training loss: 2.581386089324951\n",
      "epoch: 3020, training loss: 2.559769630432129\n",
      "epoch: 3030, training loss: 2.4879150390625\n",
      "epoch: 3040, training loss: 2.509537935256958\n",
      "epoch: 3050, training loss: 2.4678797721862793\n",
      "epoch: 3060, training loss: 2.5006608963012695\n",
      "epoch: 3070, training loss: 2.479388475418091\n",
      "epoch: 3080, training loss: 2.523792028427124\n",
      "epoch: 3090, training loss: 2.4430460929870605\n",
      "epoch: 3100, training loss: 2.493237257003784\n",
      "epoch: 3110, training loss: 2.5461668968200684\n",
      "epoch: 3120, training loss: 2.549948215484619\n",
      "epoch: 3130, training loss: 2.5377490520477295\n",
      "epoch: 3140, training loss: 2.4454450607299805\n",
      "epoch: 3150, training loss: 2.464627265930176\n",
      "epoch: 3160, training loss: 2.4591546058654785\n",
      "epoch: 3170, training loss: 2.461912155151367\n",
      "epoch: 3180, training loss: 2.4851839542388916\n",
      "epoch: 3190, training loss: 2.5477874279022217\n",
      "epoch: 3200, training loss: 2.564605712890625\n",
      "epoch: 3210, training loss: 2.606412649154663\n",
      "epoch: 3220, training loss: 2.504772424697876\n",
      "epoch: 3230, training loss: 2.500128984451294\n",
      "epoch: 3240, training loss: 2.3618664741516113\n",
      "epoch: 3250, training loss: 2.50242018699646\n",
      "epoch: 3260, training loss: 2.552457809448242\n",
      "epoch: 3270, training loss: 2.584665298461914\n",
      "epoch: 3280, training loss: 2.546515464782715\n",
      "epoch: 3290, training loss: 2.5188045501708984\n",
      "epoch: 3300, training loss: 2.4716789722442627\n",
      "epoch: 3310, training loss: 2.5114428997039795\n",
      "epoch: 3320, training loss: 2.533789873123169\n",
      "epoch: 3330, training loss: 2.4455785751342773\n",
      "epoch: 3340, training loss: 2.4562110900878906\n",
      "epoch: 3350, training loss: 2.4931249618530273\n",
      "epoch: 3360, training loss: 2.479234457015991\n",
      "epoch: 3370, training loss: 2.4894208908081055\n",
      "epoch: 3380, training loss: 2.4259963035583496\n",
      "epoch: 3390, training loss: 2.4716944694519043\n",
      "epoch: 3400, training loss: 2.4289793968200684\n",
      "epoch: 3410, training loss: 2.4424872398376465\n",
      "epoch: 3420, training loss: 2.4773848056793213\n",
      "epoch: 3430, training loss: 2.5070395469665527\n",
      "epoch: 3440, training loss: 2.439621686935425\n",
      "epoch: 3450, training loss: 2.441556215286255\n",
      "epoch: 3460, training loss: 2.60735821723938\n",
      "epoch: 3470, training loss: 2.5250771045684814\n",
      "epoch: 3480, training loss: 2.5021839141845703\n",
      "epoch: 3490, training loss: 2.5227584838867188\n",
      "epoch: 3500, training loss: 2.5286409854888916\n",
      "epoch: 3510, training loss: 2.40452241897583\n",
      "epoch: 3520, training loss: 2.4893195629119873\n",
      "epoch: 3530, training loss: 2.4910686016082764\n",
      "epoch: 3540, training loss: 2.480872631072998\n",
      "epoch: 3550, training loss: 2.5105957984924316\n",
      "epoch: 3560, training loss: 2.4833972454071045\n",
      "epoch: 3570, training loss: 2.5334925651550293\n",
      "epoch: 3580, training loss: 2.5034782886505127\n",
      "epoch: 3590, training loss: 2.4628705978393555\n",
      "epoch: 3600, training loss: 2.535562038421631\n",
      "epoch: 3610, training loss: 2.5046708583831787\n",
      "epoch: 3620, training loss: 2.505242347717285\n",
      "epoch: 3630, training loss: 2.4182627201080322\n",
      "epoch: 3640, training loss: 2.48345685005188\n",
      "epoch: 3650, training loss: 2.528567314147949\n",
      "epoch: 3660, training loss: 2.4493658542633057\n",
      "epoch: 3670, training loss: 2.559621810913086\n",
      "epoch: 3680, training loss: 2.459465503692627\n",
      "epoch: 3690, training loss: 2.388601779937744\n",
      "epoch: 3700, training loss: 2.491736650466919\n",
      "epoch: 3710, training loss: 2.4858553409576416\n",
      "epoch: 3720, training loss: 2.5035691261291504\n",
      "epoch: 3730, training loss: 2.577101469039917\n",
      "epoch: 3740, training loss: 2.4889700412750244\n",
      "epoch: 3750, training loss: 2.4268383979797363\n",
      "epoch: 3760, training loss: 2.443366289138794\n",
      "epoch: 3770, training loss: 2.4073030948638916\n",
      "epoch: 3780, training loss: 2.531836748123169\n",
      "epoch: 3790, training loss: 2.4595766067504883\n",
      "epoch: 3800, training loss: 2.464174747467041\n",
      "epoch: 3810, training loss: 2.5562596321105957\n",
      "epoch: 3820, training loss: 2.5298912525177\n",
      "epoch: 3830, training loss: 2.5168893337249756\n",
      "epoch: 3840, training loss: 2.480341911315918\n",
      "epoch: 3850, training loss: 2.551323175430298\n",
      "epoch: 3860, training loss: 2.3913393020629883\n",
      "epoch: 3870, training loss: 2.528160333633423\n",
      "epoch: 3880, training loss: 2.4805521965026855\n",
      "epoch: 3890, training loss: 2.507028818130493\n",
      "epoch: 3900, training loss: 2.4884769916534424\n",
      "epoch: 3910, training loss: 2.4524285793304443\n",
      "epoch: 3920, training loss: 2.5224242210388184\n",
      "epoch: 3930, training loss: 2.4841439723968506\n",
      "epoch: 3940, training loss: 2.5392932891845703\n",
      "epoch: 3950, training loss: 2.482509136199951\n",
      "epoch: 3960, training loss: 2.490722894668579\n",
      "epoch: 3970, training loss: 2.381465435028076\n",
      "epoch: 3980, training loss: 2.4511146545410156\n",
      "epoch: 3990, training loss: 2.5531768798828125\n",
      "epoch: 4000, training loss: 2.538294792175293\n",
      "epoch: 4010, training loss: 2.5620765686035156\n",
      "epoch: 4020, training loss: 2.4227702617645264\n",
      "epoch: 4030, training loss: 2.495246171951294\n",
      "epoch: 4040, training loss: 2.4667115211486816\n",
      "epoch: 4050, training loss: 2.5109124183654785\n",
      "epoch: 4060, training loss: 2.4825477600097656\n",
      "epoch: 4070, training loss: 2.4976816177368164\n",
      "epoch: 4080, training loss: 2.463653326034546\n",
      "epoch: 4090, training loss: 2.4785420894622803\n",
      "epoch: 4100, training loss: 2.5530121326446533\n",
      "epoch: 4110, training loss: 2.4740054607391357\n",
      "epoch: 4120, training loss: 2.5029611587524414\n",
      "epoch: 4130, training loss: 2.422257423400879\n",
      "epoch: 4140, training loss: 2.5135626792907715\n",
      "epoch: 4150, training loss: 2.4358441829681396\n",
      "epoch: 4160, training loss: 2.419558048248291\n",
      "epoch: 4170, training loss: 2.4418928623199463\n",
      "epoch: 4180, training loss: 2.399552345275879\n",
      "epoch: 4190, training loss: 2.47196364402771\n",
      "epoch: 4200, training loss: 2.4322099685668945\n",
      "epoch: 4210, training loss: 2.477714776992798\n",
      "epoch: 4220, training loss: 2.4471569061279297\n",
      "epoch: 4230, training loss: 2.5173826217651367\n",
      "epoch: 4240, training loss: 2.576030731201172\n",
      "epoch: 4250, training loss: 2.464460849761963\n",
      "epoch: 4260, training loss: 2.48740816116333\n",
      "epoch: 4270, training loss: 2.4816324710845947\n",
      "epoch: 4280, training loss: 2.4327054023742676\n",
      "epoch: 4290, training loss: 2.5290112495422363\n",
      "epoch: 4300, training loss: 2.360746145248413\n",
      "epoch: 4310, training loss: 2.5866169929504395\n",
      "epoch: 4320, training loss: 2.500990152359009\n",
      "epoch: 4330, training loss: 2.5240509510040283\n",
      "epoch: 4340, training loss: 2.5133931636810303\n",
      "epoch: 4350, training loss: 2.4361157417297363\n",
      "epoch: 4360, training loss: 2.501347064971924\n",
      "epoch: 4370, training loss: 2.413229465484619\n",
      "epoch: 4380, training loss: 2.51643967628479\n",
      "epoch: 4390, training loss: 2.450655937194824\n",
      "epoch: 4400, training loss: 2.547757863998413\n",
      "epoch: 4410, training loss: 2.4191112518310547\n",
      "epoch: 4420, training loss: 2.5261266231536865\n",
      "epoch: 4430, training loss: 2.5302116870880127\n",
      "epoch: 4440, training loss: 2.4740233421325684\n",
      "epoch: 4450, training loss: 2.450120449066162\n",
      "epoch: 4460, training loss: 2.5131537914276123\n",
      "epoch: 4470, training loss: 2.478087902069092\n",
      "epoch: 4480, training loss: 2.4985389709472656\n",
      "epoch: 4490, training loss: 2.443974018096924\n",
      "epoch: 4500, training loss: 2.5089595317840576\n",
      "epoch: 4510, training loss: 2.562044143676758\n",
      "epoch: 4520, training loss: 2.3849313259124756\n",
      "epoch: 4530, training loss: 2.4594922065734863\n",
      "epoch: 4540, training loss: 2.4673585891723633\n",
      "epoch: 4550, training loss: 2.405506134033203\n",
      "epoch: 4560, training loss: 2.469120502471924\n",
      "epoch: 4570, training loss: 2.561518430709839\n",
      "epoch: 4580, training loss: 2.43548846244812\n",
      "epoch: 4590, training loss: 2.5356698036193848\n",
      "epoch: 4600, training loss: 2.4926278591156006\n",
      "epoch: 4610, training loss: 2.4632749557495117\n",
      "epoch: 4620, training loss: 2.5028557777404785\n",
      "epoch: 4630, training loss: 2.4339540004730225\n",
      "epoch: 4640, training loss: 2.4627504348754883\n",
      "epoch: 4650, training loss: 2.4427921772003174\n",
      "epoch: 4660, training loss: 2.420254945755005\n",
      "epoch: 4670, training loss: 2.4321749210357666\n",
      "epoch: 4680, training loss: 2.4527499675750732\n",
      "epoch: 4690, training loss: 2.5405194759368896\n",
      "epoch: 4700, training loss: 2.484701156616211\n",
      "epoch: 4710, training loss: 2.4544992446899414\n",
      "epoch: 4720, training loss: 2.4555165767669678\n",
      "epoch: 4730, training loss: 2.5465810298919678\n",
      "epoch: 4740, training loss: 2.510685920715332\n",
      "epoch: 4750, training loss: 2.489086151123047\n",
      "epoch: 4760, training loss: 2.384734630584717\n",
      "epoch: 4770, training loss: 2.4532365798950195\n",
      "epoch: 4780, training loss: 2.36031174659729\n",
      "epoch: 4790, training loss: 2.5564470291137695\n",
      "epoch: 4800, training loss: 2.4784507751464844\n",
      "epoch: 4810, training loss: 2.4671247005462646\n",
      "epoch: 4820, training loss: 2.443204641342163\n",
      "epoch: 4830, training loss: 2.4920525550842285\n",
      "epoch: 4840, training loss: 2.455261707305908\n",
      "epoch: 4850, training loss: 2.429082155227661\n",
      "epoch: 4860, training loss: 2.547705888748169\n",
      "epoch: 4870, training loss: 2.5266165733337402\n",
      "epoch: 4880, training loss: 2.537599802017212\n",
      "epoch: 4890, training loss: 2.551405668258667\n",
      "epoch: 4900, training loss: 2.5069665908813477\n",
      "epoch: 4910, training loss: 2.4663360118865967\n",
      "epoch: 4920, training loss: 2.4869844913482666\n",
      "epoch: 4930, training loss: 2.4464921951293945\n",
      "epoch: 4940, training loss: 2.455010414123535\n",
      "epoch: 4950, training loss: 2.5149829387664795\n",
      "epoch: 4960, training loss: 2.4843971729278564\n",
      "epoch: 4970, training loss: 2.4865310192108154\n",
      "epoch: 4980, training loss: 2.3287885189056396\n",
      "epoch: 4990, training loss: 2.5039682388305664\n",
      "epoch: 5000, training loss: 2.4503116607666016\n",
      "epoch: 5010, training loss: 2.450523614883423\n",
      "epoch: 5020, training loss: 2.2963027954101562\n",
      "epoch: 5030, training loss: 2.5468881130218506\n",
      "epoch: 5040, training loss: 2.538140058517456\n",
      "epoch: 5050, training loss: 2.3731462955474854\n",
      "epoch: 5060, training loss: 2.486004114151001\n",
      "epoch: 5070, training loss: 2.503559112548828\n",
      "epoch: 5080, training loss: 2.459176778793335\n",
      "epoch: 5090, training loss: 2.498380422592163\n",
      "epoch: 5100, training loss: 2.4979066848754883\n",
      "epoch: 5110, training loss: 2.4553730487823486\n",
      "epoch: 5120, training loss: 2.5219314098358154\n",
      "epoch: 5130, training loss: 2.4150729179382324\n",
      "epoch: 5140, training loss: 2.4758658409118652\n",
      "epoch: 5150, training loss: 2.4894959926605225\n",
      "epoch: 5160, training loss: 2.518372058868408\n",
      "epoch: 5170, training loss: 2.5156688690185547\n",
      "epoch: 5180, training loss: 2.436683177947998\n",
      "epoch: 5190, training loss: 2.359192132949829\n",
      "epoch: 5200, training loss: 2.464484930038452\n",
      "epoch: 5210, training loss: 2.5202388763427734\n",
      "epoch: 5220, training loss: 2.5050463676452637\n",
      "epoch: 5230, training loss: 2.505279064178467\n",
      "epoch: 5240, training loss: 2.4656662940979004\n",
      "epoch: 5250, training loss: 2.5862536430358887\n",
      "epoch: 5260, training loss: 2.4515888690948486\n",
      "epoch: 5270, training loss: 2.4699854850769043\n",
      "epoch: 5280, training loss: 2.4869396686553955\n",
      "epoch: 5290, training loss: 2.458869695663452\n",
      "epoch: 5300, training loss: 2.4940614700317383\n",
      "epoch: 5310, training loss: 2.5239641666412354\n",
      "epoch: 5320, training loss: 2.5032575130462646\n",
      "epoch: 5330, training loss: 2.4552900791168213\n",
      "epoch: 5340, training loss: 2.517202615737915\n",
      "epoch: 5350, training loss: 2.4966940879821777\n",
      "epoch: 5360, training loss: 2.4381494522094727\n",
      "epoch: 5370, training loss: 2.484315872192383\n",
      "epoch: 5380, training loss: 2.3696305751800537\n",
      "epoch: 5390, training loss: 2.4100799560546875\n",
      "epoch: 5400, training loss: 2.466440200805664\n",
      "epoch: 5410, training loss: 2.442600727081299\n",
      "epoch: 5420, training loss: 2.446108102798462\n",
      "epoch: 5430, training loss: 2.4568347930908203\n",
      "epoch: 5440, training loss: 2.478210687637329\n",
      "epoch: 5450, training loss: 2.4716696739196777\n",
      "epoch: 5460, training loss: 2.4267499446868896\n",
      "epoch: 5470, training loss: 2.441028118133545\n",
      "epoch: 5480, training loss: 2.414879560470581\n",
      "epoch: 5490, training loss: 2.45715594291687\n",
      "epoch: 5500, training loss: 2.547023296356201\n",
      "epoch: 5510, training loss: 2.384037733078003\n",
      "epoch: 5520, training loss: 2.5089690685272217\n",
      "epoch: 5530, training loss: 2.3946049213409424\n",
      "epoch: 5540, training loss: 2.5028793811798096\n",
      "epoch: 5550, training loss: 2.365034341812134\n",
      "epoch: 5560, training loss: 2.5037996768951416\n",
      "epoch: 5570, training loss: 2.526721477508545\n",
      "epoch: 5580, training loss: 2.471632242202759\n",
      "epoch: 5590, training loss: 2.483018636703491\n",
      "epoch: 5600, training loss: 2.5319578647613525\n",
      "epoch: 5610, training loss: 2.4451091289520264\n",
      "epoch: 5620, training loss: 2.505277633666992\n",
      "epoch: 5630, training loss: 2.4777657985687256\n",
      "epoch: 5640, training loss: 2.5302321910858154\n",
      "epoch: 5650, training loss: 2.437272787094116\n",
      "epoch: 5660, training loss: 2.4921162128448486\n",
      "epoch: 5670, training loss: 2.509219169616699\n",
      "epoch: 5680, training loss: 2.527451753616333\n",
      "epoch: 5690, training loss: 2.429625988006592\n",
      "epoch: 5700, training loss: 2.438483476638794\n",
      "epoch: 5710, training loss: 2.399153232574463\n",
      "epoch: 5720, training loss: 2.4021177291870117\n",
      "epoch: 5730, training loss: 2.4655983448028564\n",
      "epoch: 5740, training loss: 2.4118337631225586\n",
      "epoch: 5750, training loss: 2.5358963012695312\n",
      "epoch: 5760, training loss: 2.5611190795898438\n",
      "epoch: 5770, training loss: 2.406860589981079\n",
      "epoch: 5780, training loss: 2.472583532333374\n",
      "epoch: 5790, training loss: 2.5962352752685547\n",
      "epoch: 5800, training loss: 2.4264841079711914\n",
      "epoch: 5810, training loss: 2.4556593894958496\n",
      "epoch: 5820, training loss: 2.448126792907715\n",
      "epoch: 5830, training loss: 2.439490795135498\n",
      "epoch: 5840, training loss: 2.5098516941070557\n",
      "epoch: 5850, training loss: 2.507143497467041\n",
      "epoch: 5860, training loss: 2.455014705657959\n",
      "epoch: 5870, training loss: 2.510909080505371\n",
      "epoch: 5880, training loss: 2.568004846572876\n",
      "epoch: 5890, training loss: 2.450042963027954\n",
      "epoch: 5900, training loss: 2.5680034160614014\n",
      "epoch: 5910, training loss: 2.3820302486419678\n",
      "epoch: 5920, training loss: 2.523073196411133\n",
      "epoch: 5930, training loss: 2.3897640705108643\n",
      "epoch: 5940, training loss: 2.4257795810699463\n",
      "epoch: 5950, training loss: 2.5187628269195557\n",
      "epoch: 5960, training loss: 2.4337737560272217\n",
      "epoch: 5970, training loss: 2.413588762283325\n",
      "epoch: 5980, training loss: 2.4452075958251953\n",
      "epoch: 5990, training loss: 2.4512805938720703\n",
      "epoch: 6000, training loss: 2.3985776901245117\n",
      "epoch: 6010, training loss: 2.4617767333984375\n",
      "epoch: 6020, training loss: 2.4673354625701904\n",
      "epoch: 6030, training loss: 2.445845365524292\n",
      "epoch: 6040, training loss: 2.4529569149017334\n",
      "epoch: 6050, training loss: 2.4956436157226562\n",
      "epoch: 6060, training loss: 2.5948283672332764\n",
      "epoch: 6070, training loss: 2.3694417476654053\n",
      "epoch: 6080, training loss: 2.5322399139404297\n",
      "epoch: 6090, training loss: 2.445387125015259\n",
      "epoch: 6100, training loss: 2.5267910957336426\n",
      "epoch: 6110, training loss: 2.5114731788635254\n",
      "epoch: 6120, training loss: 2.4606945514678955\n",
      "epoch: 6130, training loss: 2.4827589988708496\n",
      "epoch: 6140, training loss: 2.4799118041992188\n",
      "epoch: 6150, training loss: 2.5596725940704346\n",
      "epoch: 6160, training loss: 2.4161274433135986\n",
      "epoch: 6170, training loss: 2.4888291358947754\n",
      "epoch: 6180, training loss: 2.5121665000915527\n",
      "epoch: 6190, training loss: 2.4648280143737793\n",
      "epoch: 6200, training loss: 2.3870511054992676\n",
      "epoch: 6210, training loss: 2.5221445560455322\n",
      "epoch: 6220, training loss: 2.4549829959869385\n",
      "epoch: 6230, training loss: 2.4122519493103027\n",
      "epoch: 6240, training loss: 2.4286134243011475\n",
      "epoch: 6250, training loss: 2.45780348777771\n",
      "epoch: 6260, training loss: 2.461331605911255\n",
      "epoch: 6270, training loss: 2.436047077178955\n",
      "epoch: 6280, training loss: 2.421623945236206\n",
      "epoch: 6290, training loss: 2.428847074508667\n",
      "epoch: 6300, training loss: 2.4579811096191406\n",
      "epoch: 6310, training loss: 2.4812960624694824\n",
      "epoch: 6320, training loss: 2.509675979614258\n",
      "epoch: 6330, training loss: 2.3575351238250732\n",
      "epoch: 6340, training loss: 2.5089008808135986\n",
      "epoch: 6350, training loss: 2.5055484771728516\n",
      "epoch: 6360, training loss: 2.3364710807800293\n",
      "epoch: 6370, training loss: 2.3947396278381348\n",
      "epoch: 6380, training loss: 2.386348247528076\n",
      "epoch: 6390, training loss: 2.539569616317749\n",
      "epoch: 6400, training loss: 2.482255697250366\n",
      "epoch: 6410, training loss: 2.4789886474609375\n",
      "epoch: 6420, training loss: 2.5493717193603516\n",
      "epoch: 6430, training loss: 2.4694039821624756\n",
      "epoch: 6440, training loss: 2.4227354526519775\n",
      "epoch: 6450, training loss: 2.4901466369628906\n",
      "epoch: 6460, training loss: 2.40797758102417\n",
      "epoch: 6470, training loss: 2.452085018157959\n",
      "epoch: 6480, training loss: 2.4917800426483154\n",
      "epoch: 6490, training loss: 2.465447425842285\n",
      "epoch: 6500, training loss: 2.4515819549560547\n",
      "epoch: 6510, training loss: 2.4886317253112793\n",
      "epoch: 6520, training loss: 2.586432456970215\n",
      "epoch: 6530, training loss: 2.49381685256958\n",
      "epoch: 6540, training loss: 2.459650754928589\n",
      "epoch: 6550, training loss: 2.564934492111206\n",
      "epoch: 6560, training loss: 2.4608447551727295\n",
      "epoch: 6570, training loss: 2.4511892795562744\n",
      "epoch: 6580, training loss: 2.4423797130584717\n",
      "epoch: 6590, training loss: 2.442899465560913\n",
      "epoch: 6600, training loss: 2.47942852973938\n",
      "epoch: 6610, training loss: 2.4883930683135986\n",
      "epoch: 6620, training loss: 2.399765968322754\n",
      "epoch: 6630, training loss: 2.5147056579589844\n",
      "epoch: 6640, training loss: 2.4732296466827393\n",
      "epoch: 6650, training loss: 2.5392045974731445\n",
      "epoch: 6660, training loss: 2.4729175567626953\n",
      "epoch: 6670, training loss: 2.4634265899658203\n",
      "epoch: 6680, training loss: 2.4081568717956543\n",
      "epoch: 6690, training loss: 2.3838655948638916\n",
      "epoch: 6700, training loss: 2.400148630142212\n",
      "epoch: 6710, training loss: 2.4634485244750977\n",
      "epoch: 6720, training loss: 2.451763153076172\n",
      "epoch: 6730, training loss: 2.4652862548828125\n",
      "epoch: 6740, training loss: 2.4863803386688232\n",
      "epoch: 6750, training loss: 2.480424165725708\n",
      "epoch: 6760, training loss: 2.5404441356658936\n",
      "epoch: 6770, training loss: 2.3987879753112793\n",
      "epoch: 6780, training loss: 2.5299580097198486\n",
      "epoch: 6790, training loss: 2.543231248855591\n",
      "epoch: 6800, training loss: 2.466895341873169\n",
      "epoch: 6810, training loss: 2.4951400756835938\n",
      "epoch: 6820, training loss: 2.478393077850342\n",
      "epoch: 6830, training loss: 2.4443209171295166\n",
      "epoch: 6840, training loss: 2.4080111980438232\n",
      "epoch: 6850, training loss: 2.4885478019714355\n",
      "epoch: 6860, training loss: 2.5255775451660156\n",
      "epoch: 6870, training loss: 2.4382436275482178\n",
      "epoch: 6880, training loss: 2.409965753555298\n",
      "epoch: 6890, training loss: 2.412911891937256\n",
      "epoch: 6900, training loss: 2.462627649307251\n",
      "epoch: 6910, training loss: 2.4554004669189453\n",
      "epoch: 6920, training loss: 2.4513332843780518\n",
      "epoch: 6930, training loss: 2.5056166648864746\n",
      "epoch: 6940, training loss: 2.4468886852264404\n",
      "epoch: 6950, training loss: 2.4570870399475098\n",
      "epoch: 6960, training loss: 2.4730653762817383\n",
      "epoch: 6970, training loss: 2.400171995162964\n",
      "epoch: 6980, training loss: 2.4220244884490967\n",
      "epoch: 6990, training loss: 2.4621455669403076\n",
      "epoch: 7000, training loss: 2.3901453018188477\n",
      "epoch: 7010, training loss: 2.4059977531433105\n",
      "epoch: 7020, training loss: 2.4880363941192627\n",
      "epoch: 7030, training loss: 2.4429545402526855\n",
      "epoch: 7040, training loss: 2.4562642574310303\n",
      "epoch: 7050, training loss: 2.5019195079803467\n",
      "epoch: 7060, training loss: 2.3772547245025635\n",
      "epoch: 7070, training loss: 2.39502215385437\n",
      "epoch: 7080, training loss: 2.480351686477661\n",
      "epoch: 7090, training loss: 2.5193324089050293\n",
      "epoch: 7100, training loss: 2.4325716495513916\n",
      "epoch: 7110, training loss: 2.4456117153167725\n",
      "epoch: 7120, training loss: 2.3854877948760986\n",
      "epoch: 7130, training loss: 2.498750686645508\n",
      "epoch: 7140, training loss: 2.451251983642578\n",
      "epoch: 7150, training loss: 2.4458320140838623\n",
      "epoch: 7160, training loss: 2.4511051177978516\n",
      "epoch: 7170, training loss: 2.4334638118743896\n",
      "epoch: 7180, training loss: 2.4910085201263428\n",
      "epoch: 7190, training loss: 2.4730255603790283\n",
      "epoch: 7200, training loss: 2.41349458694458\n",
      "epoch: 7210, training loss: 2.4138450622558594\n",
      "epoch: 7220, training loss: 2.496358871459961\n",
      "epoch: 7230, training loss: 2.527247905731201\n",
      "epoch: 7240, training loss: 2.447953462600708\n",
      "epoch: 7250, training loss: 2.4559884071350098\n",
      "epoch: 7260, training loss: 2.4145514965057373\n",
      "epoch: 7270, training loss: 2.4375247955322266\n",
      "epoch: 7280, training loss: 2.452986240386963\n",
      "epoch: 7290, training loss: 2.407400131225586\n",
      "epoch: 7300, training loss: 2.500852346420288\n",
      "epoch: 7310, training loss: 2.486316680908203\n",
      "epoch: 7320, training loss: 2.4221692085266113\n",
      "epoch: 7330, training loss: 2.4692862033843994\n",
      "epoch: 7340, training loss: 2.5289645195007324\n",
      "epoch: 7350, training loss: 2.4525110721588135\n",
      "epoch: 7360, training loss: 2.4687352180480957\n",
      "epoch: 7370, training loss: 2.5328009128570557\n",
      "epoch: 7380, training loss: 2.459587574005127\n",
      "epoch: 7390, training loss: 2.4552369117736816\n",
      "epoch: 7400, training loss: 2.4161458015441895\n",
      "epoch: 7410, training loss: 2.493981122970581\n",
      "epoch: 7420, training loss: 2.475588083267212\n",
      "epoch: 7430, training loss: 2.3880255222320557\n",
      "epoch: 7440, training loss: 2.4326632022857666\n",
      "epoch: 7450, training loss: 2.4859189987182617\n",
      "epoch: 7460, training loss: 2.3602826595306396\n",
      "epoch: 7470, training loss: 2.4907374382019043\n",
      "epoch: 7480, training loss: 2.41861891746521\n",
      "epoch: 7490, training loss: 2.4171760082244873\n",
      "epoch: 7500, training loss: 2.4867496490478516\n",
      "epoch: 7510, training loss: 2.4312310218811035\n",
      "epoch: 7520, training loss: 2.3803606033325195\n",
      "epoch: 7530, training loss: 2.517705202102661\n",
      "epoch: 7540, training loss: 2.4506428241729736\n",
      "epoch: 7550, training loss: 2.4613211154937744\n",
      "epoch: 7560, training loss: 2.4922494888305664\n",
      "epoch: 7570, training loss: 2.5148167610168457\n",
      "epoch: 7580, training loss: 2.4259510040283203\n",
      "epoch: 7590, training loss: 2.408885955810547\n",
      "epoch: 7600, training loss: 2.4501402378082275\n",
      "epoch: 7610, training loss: 2.3348283767700195\n",
      "epoch: 7620, training loss: 2.4407501220703125\n",
      "epoch: 7630, training loss: 2.4169223308563232\n",
      "epoch: 7640, training loss: 2.5320632457733154\n",
      "epoch: 7650, training loss: 2.4239470958709717\n",
      "epoch: 7660, training loss: 2.529390811920166\n",
      "epoch: 7670, training loss: 2.445578098297119\n",
      "epoch: 7680, training loss: 2.49688458442688\n",
      "epoch: 7690, training loss: 2.410909652709961\n",
      "epoch: 7700, training loss: 2.4610211849212646\n",
      "epoch: 7710, training loss: 2.4843175411224365\n",
      "epoch: 7720, training loss: 2.515698194503784\n",
      "epoch: 7730, training loss: 2.3947701454162598\n",
      "epoch: 7740, training loss: 2.4241135120391846\n",
      "epoch: 7750, training loss: 2.5407111644744873\n",
      "epoch: 7760, training loss: 2.4999868869781494\n",
      "epoch: 7770, training loss: 2.4942567348480225\n",
      "epoch: 7780, training loss: 2.477169990539551\n",
      "epoch: 7790, training loss: 2.532792806625366\n",
      "epoch: 7800, training loss: 2.5088188648223877\n",
      "epoch: 7810, training loss: 2.412209987640381\n",
      "epoch: 7820, training loss: 2.471842050552368\n",
      "epoch: 7830, training loss: 2.512227773666382\n",
      "epoch: 7840, training loss: 2.4963557720184326\n",
      "epoch: 7850, training loss: 2.4068901538848877\n",
      "epoch: 7860, training loss: 2.517805337905884\n",
      "epoch: 7870, training loss: 2.476922035217285\n",
      "epoch: 7880, training loss: 2.4508748054504395\n",
      "epoch: 7890, training loss: 2.4332587718963623\n",
      "epoch: 7900, training loss: 2.5093014240264893\n",
      "epoch: 7910, training loss: 2.4175424575805664\n",
      "epoch: 7920, training loss: 2.47135066986084\n",
      "epoch: 7930, training loss: 2.491133689880371\n",
      "epoch: 7940, training loss: 2.488032579421997\n",
      "epoch: 7950, training loss: 2.4759063720703125\n",
      "epoch: 7960, training loss: 2.4712517261505127\n",
      "epoch: 7970, training loss: 2.426715135574341\n",
      "epoch: 7980, training loss: 2.5079843997955322\n",
      "epoch: 7990, training loss: 2.393604278564453\n",
      "epoch: 8000, training loss: 2.44063401222229\n",
      "epoch: 8010, training loss: 2.4624078273773193\n",
      "epoch: 8020, training loss: 2.50936222076416\n",
      "epoch: 8030, training loss: 2.411329507827759\n",
      "epoch: 8040, training loss: 2.3814291954040527\n",
      "epoch: 8050, training loss: 2.451451539993286\n",
      "epoch: 8060, training loss: 2.488964080810547\n",
      "epoch: 8070, training loss: 2.363830327987671\n",
      "epoch: 8080, training loss: 2.3827743530273438\n",
      "epoch: 8090, training loss: 2.447000741958618\n",
      "epoch: 8100, training loss: 2.473375082015991\n",
      "epoch: 8110, training loss: 2.486147403717041\n",
      "epoch: 8120, training loss: 2.4274802207946777\n",
      "epoch: 8130, training loss: 2.436474084854126\n",
      "epoch: 8140, training loss: 2.429670810699463\n",
      "epoch: 8150, training loss: 2.4809651374816895\n",
      "epoch: 8160, training loss: 2.4349148273468018\n",
      "epoch: 8170, training loss: 2.4520087242126465\n",
      "epoch: 8180, training loss: 2.437635898590088\n",
      "epoch: 8190, training loss: 2.42047381401062\n",
      "epoch: 8200, training loss: 2.4752886295318604\n",
      "epoch: 8210, training loss: 2.50891375541687\n",
      "epoch: 8220, training loss: 2.4773635864257812\n",
      "epoch: 8230, training loss: 2.413020133972168\n",
      "epoch: 8240, training loss: 2.4943511486053467\n",
      "epoch: 8250, training loss: 2.378528356552124\n",
      "epoch: 8260, training loss: 2.4264330863952637\n",
      "epoch: 8270, training loss: 2.4512839317321777\n",
      "epoch: 8280, training loss: 2.436980724334717\n",
      "epoch: 8290, training loss: 2.4343676567077637\n",
      "epoch: 8300, training loss: 2.4593513011932373\n",
      "epoch: 8310, training loss: 2.450817584991455\n",
      "epoch: 8320, training loss: 2.5183825492858887\n",
      "epoch: 8330, training loss: 2.5065970420837402\n",
      "epoch: 8340, training loss: 2.494265556335449\n",
      "epoch: 8350, training loss: 2.4454236030578613\n",
      "epoch: 8360, training loss: 2.548959493637085\n",
      "epoch: 8370, training loss: 2.4084372520446777\n",
      "epoch: 8380, training loss: 2.413759231567383\n",
      "epoch: 8390, training loss: 2.447962999343872\n",
      "epoch: 8400, training loss: 2.436300277709961\n",
      "epoch: 8410, training loss: 2.449366331100464\n",
      "epoch: 8420, training loss: 2.4615066051483154\n",
      "epoch: 8430, training loss: 2.4170236587524414\n",
      "epoch: 8440, training loss: 2.3897287845611572\n",
      "epoch: 8450, training loss: 2.397411823272705\n",
      "epoch: 8460, training loss: 2.453110456466675\n",
      "epoch: 8470, training loss: 2.5617237091064453\n",
      "epoch: 8480, training loss: 2.470675468444824\n",
      "epoch: 8490, training loss: 2.4414546489715576\n",
      "epoch: 8500, training loss: 2.469550132751465\n",
      "epoch: 8510, training loss: 2.377157211303711\n",
      "epoch: 8520, training loss: 2.4208824634552\n",
      "epoch: 8530, training loss: 2.4973127841949463\n",
      "epoch: 8540, training loss: 2.5638039112091064\n",
      "epoch: 8550, training loss: 2.4433412551879883\n",
      "epoch: 8560, training loss: 2.434866189956665\n",
      "epoch: 8570, training loss: 2.5475785732269287\n",
      "epoch: 8580, training loss: 2.486984968185425\n",
      "epoch: 8590, training loss: 2.4930288791656494\n",
      "epoch: 8600, training loss: 2.451637029647827\n",
      "epoch: 8610, training loss: 2.4707298278808594\n",
      "epoch: 8620, training loss: 2.4815127849578857\n",
      "epoch: 8630, training loss: 2.467961072921753\n",
      "epoch: 8640, training loss: 2.46038818359375\n",
      "epoch: 8650, training loss: 2.446226119995117\n",
      "epoch: 8660, training loss: 2.5746302604675293\n",
      "epoch: 8670, training loss: 2.4553372859954834\n",
      "epoch: 8680, training loss: 2.349642515182495\n",
      "epoch: 8690, training loss: 2.4630661010742188\n",
      "epoch: 8700, training loss: 2.545968532562256\n",
      "epoch: 8710, training loss: 2.474327564239502\n",
      "epoch: 8720, training loss: 2.4219746589660645\n",
      "epoch: 8730, training loss: 2.491082191467285\n",
      "epoch: 8740, training loss: 2.4499711990356445\n",
      "epoch: 8750, training loss: 2.4658360481262207\n",
      "epoch: 8760, training loss: 2.4372780323028564\n",
      "epoch: 8770, training loss: 2.3709306716918945\n",
      "epoch: 8780, training loss: 2.4517605304718018\n",
      "epoch: 8790, training loss: 2.443557024002075\n",
      "epoch: 8800, training loss: 2.536851167678833\n",
      "epoch: 8810, training loss: 2.4935808181762695\n",
      "epoch: 8820, training loss: 2.458538770675659\n",
      "epoch: 8830, training loss: 2.4387972354888916\n",
      "epoch: 8840, training loss: 2.4862582683563232\n",
      "epoch: 8850, training loss: 2.490976572036743\n",
      "epoch: 8860, training loss: 2.4619405269622803\n",
      "epoch: 8870, training loss: 2.4724042415618896\n",
      "epoch: 8880, training loss: 2.5871567726135254\n",
      "epoch: 8890, training loss: 2.4252889156341553\n",
      "epoch: 8900, training loss: 2.5035390853881836\n",
      "epoch: 8910, training loss: 2.435269832611084\n",
      "epoch: 8920, training loss: 2.479011297225952\n",
      "epoch: 8930, training loss: 2.435777187347412\n",
      "epoch: 8940, training loss: 2.383378028869629\n",
      "epoch: 8950, training loss: 2.442805767059326\n",
      "epoch: 8960, training loss: 2.475112199783325\n",
      "epoch: 8970, training loss: 2.4515395164489746\n",
      "epoch: 8980, training loss: 2.4788119792938232\n",
      "epoch: 8990, training loss: 2.469604730606079\n",
      "epoch: 9000, training loss: 2.444445848464966\n",
      "epoch: 9010, training loss: 2.43751859664917\n",
      "epoch: 9020, training loss: 2.4719786643981934\n",
      "epoch: 9030, training loss: 2.4162282943725586\n",
      "epoch: 9040, training loss: 2.4451565742492676\n",
      "epoch: 9050, training loss: 2.4504778385162354\n",
      "epoch: 9060, training loss: 2.44303560256958\n",
      "epoch: 9070, training loss: 2.4638140201568604\n",
      "epoch: 9080, training loss: 2.4856717586517334\n",
      "epoch: 9090, training loss: 2.4636642932891846\n",
      "epoch: 9100, training loss: 2.493208169937134\n",
      "epoch: 9110, training loss: 2.3714921474456787\n",
      "epoch: 9120, training loss: 2.4513328075408936\n",
      "epoch: 9130, training loss: 2.53145170211792\n",
      "epoch: 9140, training loss: 2.47331166267395\n",
      "epoch: 9150, training loss: 2.480727195739746\n",
      "epoch: 9160, training loss: 2.499717950820923\n",
      "epoch: 9170, training loss: 2.4240729808807373\n",
      "epoch: 9180, training loss: 2.43583607673645\n",
      "epoch: 9190, training loss: 2.4997398853302\n",
      "epoch: 9200, training loss: 2.4608960151672363\n",
      "epoch: 9210, training loss: 2.6184864044189453\n",
      "epoch: 9220, training loss: 2.5103042125701904\n",
      "epoch: 9230, training loss: 2.5110855102539062\n",
      "epoch: 9240, training loss: 2.3862392902374268\n",
      "epoch: 9250, training loss: 2.4312968254089355\n",
      "epoch: 9260, training loss: 2.4971580505371094\n",
      "epoch: 9270, training loss: 2.4628264904022217\n",
      "epoch: 9280, training loss: 2.4239256381988525\n",
      "epoch: 9290, training loss: 2.528926372528076\n",
      "epoch: 9300, training loss: 2.4651899337768555\n",
      "epoch: 9310, training loss: 2.342040538787842\n",
      "epoch: 9320, training loss: 2.4444849491119385\n",
      "epoch: 9330, training loss: 2.4513049125671387\n",
      "epoch: 9340, training loss: 2.4697256088256836\n",
      "epoch: 9350, training loss: 2.4867165088653564\n",
      "epoch: 9360, training loss: 2.3986685276031494\n",
      "epoch: 9370, training loss: 2.4324252605438232\n",
      "epoch: 9380, training loss: 2.4990718364715576\n",
      "epoch: 9390, training loss: 2.4538543224334717\n",
      "epoch: 9400, training loss: 2.4496023654937744\n",
      "epoch: 9410, training loss: 2.354656457901001\n",
      "epoch: 9420, training loss: 2.4362597465515137\n",
      "epoch: 9430, training loss: 2.536280632019043\n",
      "epoch: 9440, training loss: 2.546823024749756\n",
      "epoch: 9450, training loss: 2.433290481567383\n",
      "epoch: 9460, training loss: 2.420478343963623\n",
      "epoch: 9470, training loss: 2.3700129985809326\n",
      "epoch: 9480, training loss: 2.41583251953125\n",
      "epoch: 9490, training loss: 2.4325106143951416\n",
      "epoch: 9500, training loss: 2.4350273609161377\n",
      "epoch: 9510, training loss: 2.475297212600708\n",
      "epoch: 9520, training loss: 2.528721809387207\n",
      "epoch: 9530, training loss: 2.39439058303833\n",
      "epoch: 9540, training loss: 2.432619094848633\n",
      "epoch: 9550, training loss: 2.4268391132354736\n",
      "epoch: 9560, training loss: 2.4264039993286133\n",
      "epoch: 9570, training loss: 2.5465445518493652\n",
      "epoch: 9580, training loss: 2.474294424057007\n",
      "epoch: 9590, training loss: 2.456906795501709\n",
      "epoch: 9600, training loss: 2.4371683597564697\n",
      "epoch: 9610, training loss: 2.448350667953491\n",
      "epoch: 9620, training loss: 2.3895106315612793\n",
      "epoch: 9630, training loss: 2.3971753120422363\n",
      "epoch: 9640, training loss: 2.4896397590637207\n",
      "epoch: 9650, training loss: 2.4315972328186035\n",
      "epoch: 9660, training loss: 2.4895172119140625\n",
      "epoch: 9670, training loss: 2.417955160140991\n",
      "epoch: 9680, training loss: 2.5744988918304443\n",
      "epoch: 9690, training loss: 2.34037709236145\n",
      "epoch: 9700, training loss: 2.486750602722168\n",
      "epoch: 9710, training loss: 2.4580469131469727\n",
      "epoch: 9720, training loss: 2.4409756660461426\n",
      "epoch: 9730, training loss: 2.3646748065948486\n",
      "epoch: 9740, training loss: 2.4724466800689697\n",
      "epoch: 9750, training loss: 2.325395345687866\n",
      "epoch: 9760, training loss: 2.4513485431671143\n",
      "epoch: 9770, training loss: 2.388101816177368\n",
      "epoch: 9780, training loss: 2.4967312812805176\n",
      "epoch: 9790, training loss: 2.4209885597229004\n",
      "epoch: 9800, training loss: 2.391672372817993\n",
      "epoch: 9810, training loss: 2.4273386001586914\n",
      "epoch: 9820, training loss: 2.519793748855591\n",
      "epoch: 9830, training loss: 2.532895803451538\n",
      "epoch: 9840, training loss: 2.3766417503356934\n",
      "epoch: 9850, training loss: 2.413693428039551\n",
      "epoch: 9860, training loss: 2.52038311958313\n",
      "epoch: 9870, training loss: 2.4451889991760254\n",
      "epoch: 9880, training loss: 2.328066110610962\n",
      "epoch: 9890, training loss: 2.4600741863250732\n",
      "epoch: 9900, training loss: 2.515939950942993\n",
      "epoch: 9910, training loss: 2.4339849948883057\n",
      "epoch: 9920, training loss: 2.461690664291382\n",
      "epoch: 9930, training loss: 2.416862964630127\n",
      "epoch: 9940, training loss: 2.435425281524658\n",
      "epoch: 9950, training loss: 2.451066255569458\n",
      "epoch: 9960, training loss: 2.364924907684326\n",
      "epoch: 9970, training loss: 2.4730582237243652\n",
      "epoch: 9980, training loss: 2.447566270828247\n",
      "epoch: 9990, training loss: 2.38716459274292\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 10000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # sample a batch of trainin data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    _, loss = m(xb, yb)\n",
    "    # reset parameter gradients\n",
    "    optimizer.zero_grad(set_to_none=True) \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"epoch: {epoch}, training loss: {loss.item()}\")    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try generating some text using the trained bigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sequence:\n",
      " \n",
      "ANougeng IISEO, k w's wet\n",
      "Thea than f as at'lathe moonouroree St t, do copous ET:\n",
      "fty is?\n",
      "FO:\n",
      "QUThey, thothe ye o s agowir pre at ge'dnd as,\n",
      "OLonkerul amed\n",
      "HE em ld t bus IZAy itoresisblat ind, man t: mys mary\n",
      "Is; tee,\n",
      "Thareel spr bes macKIst he y gbar\n",
      "QUntled wh: gu; rd methobr wiu.\n",
      "\n",
      "S:\n",
      "\n",
      "Anseve pes\n"
     ]
    }
   ],
   "source": [
    "# generate a single sequences using the model with start token 0\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "generated_seq = m.generate(idx, max_new_tokens=300)[0].tolist()\n",
    "# Decode integer tokens into characters\n",
    "generated_seq = decode(generated_seq)\n",
    "print(\"\\nGenerated sequence:\\n\",\"\".join(generated_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better! It has similar syntactic structure as the training text and even has some correct words. The quality is still very bad because the context window is too small, only the previous character is used to predict the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
